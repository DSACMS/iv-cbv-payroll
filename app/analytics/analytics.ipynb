{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7f25b-66a9-43be-8c63-b0bbdc6b425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ------------\n",
    "# INSTRUCTIONS\n",
    "#\n",
    "# Set the FROM_DATE and TO_DATE variables to define the frame of data you want to load.\n",
    "# Then, run this cell. The first time you run it, it will get all the Mixpanel events\n",
    "# in the time range, then it will save that data to a json file in /app/analytics/.\n",
    "# The next time you run this cell, it will load from the json file (unless you change the dates).\n",
    "#\n",
    "# ------------\n",
    "# Helpful Dates\n",
    "#\n",
    "# Completed pilots:\n",
    "#   LA LWC May run:           2025-05-18 - 2025-06-30\n",
    "#   AZ Constrained MAC Pilot: 2025-06-13 - 2025-08-13 7:29pm MST\n",
    "# Incomplete pilots:\n",
    "#   LA LWC August Run:        2025-08-17            - ??? Final day to query before publish date of report\n",
    "#   AZ Expanded MAC Pilot:    2025-08-13 7:30pm MST - ??? Final day to query before publish date of report\n",
    "#\n",
    "# -------------\n",
    "FROM_DATE = '2025-05-18'\n",
    "TO_DATE = '2025-06-30'\n",
    "CLIENT_AGENCY = 'la_ldh'\n",
    "FORCE_RELOAD_FROM_API = False\n",
    "\n",
    "def fetch_mixpanel_data_from_api(params):\n",
    "    API_ENDPOINT = 'https://data.mixpanel.com/api/2.0/export'\n",
    "\n",
    "    try:\n",
    "        project_root = os.path.dirname(os.getcwd())\n",
    "        dotenv_path = os.path.join(project_root, '.env.local')\n",
    "        load_dotenv(dotenv_path=dotenv_path)\n",
    "        \n",
    "        SA_USERNAME = os.environ[\"MIXPANEL_SERVICE_ACCOUNT_USERNAME\"].strip()\n",
    "        SA_SECRET = os.environ[\"MIXPANEL_SERVICE_ACCOUNT_SECRET\"].strip()\n",
    "        PROJECT_ID = os.environ[\"MIXPANEL_PROJECT_ID\"].strip()\n",
    "        AUTH_CREDS = (SA_USERNAME, SA_SECRET)\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Environment variable {e} not found. Please check your .env.local file.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    params['project_id'] = PROJECT_ID\n",
    "    \n",
    "    print(\"Fetching data from Mixpanel API...\")\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            API_ENDPOINT,\n",
    "            headers={\"accept\": \"text/plain\"},\n",
    "            params=params,\n",
    "            auth=AUTH_CREDS\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        print(\"Successfully fetched data from API.\")\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: API request failed. {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "def parse_raw_data_to_df(raw_text):\n",
    "    \"\"\"Parses newline-delimited JSON text into a DataFrame.\"\"\"\n",
    "    raw_data = []\n",
    "    for line in raw_text.strip().split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            raw_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not decode line: {line}\", file=sys.stderr)\n",
    "    \n",
    "    if not raw_data:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return pd.DataFrame(raw_data)\n",
    "\n",
    "def deduplicate_events(df):\n",
    "    \"\"\"\n",
    "    De-duplicates a DataFrame of Mixpanel events, keeping the latest event per $insert_id.\n",
    "    See https://developer.mixpanel.com/reference/event-deduplication\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    print(f\"Original event count: {len(df)}\")\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(\n",
    "        df['properties'].apply(lambda p: p.get('time') or p.get('timestamp')),\n",
    "        unit='s'\n",
    "    )\n",
    "    df['$insert_id'] = df['properties'].apply(lambda p: p.get('$insert_id'))\n",
    "    \n",
    "    # Drop rows where '$insert_id' is missing, as they cannot be de-duplicated\n",
    "    df.dropna(subset=['$insert_id'], inplace=True)\n",
    "    \n",
    "    df.sort_values('timestamp', inplace=True)\n",
    "    df.drop_duplicates(subset=['$insert_id'], keep='last', inplace=True)\n",
    "    df.drop(columns=['$insert_id'], inplace=True)\n",
    "    \n",
    "    print(f\"Event count after de-duplication: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "def get_mixpanel_data(from_date, to_date, force_reload=False):\n",
    "    \"\"\"\n",
    "    Loads de-duplicated Mixpanel data from a local file if it exists,\n",
    "    otherwise fetches, processes, de-duplicates, and saves it.\n",
    "    \"\"\"\n",
    "    file_name = f\"mixpanel_data_{from_date}_to_{to_date}.json\"\n",
    "    \n",
    "    if os.path.exists(file_name) and not force_reload:\n",
    "        print(f\"Loading de-duplicated data from local file: {file_name}\")\n",
    "        clean_df = pd.read_json(file_name, orient='records', lines=True)\n",
    "        return clean_df\n",
    "\n",
    "    # Fetch data from API\n",
    "    params = {'from_date': from_date, 'to_date': to_date}\n",
    "    raw_text = fetch_mixpanel_data_from_api(params)\n",
    "    \n",
    "    if raw_text is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = parse_raw_data_to_df(raw_text)\n",
    "    clean_df = deduplicate_events(df)\n",
    "    \n",
    "    # Save the de-duplicated data for future use\n",
    "    if not clean_df.empty:\n",
    "        clean_df.to_json(file_name, orient='records', lines=True, date_format='iso')\n",
    "        print(f\"Clean, de-duplicated data saved to {file_name}\")\n",
    "        \n",
    "    return clean_df\n",
    "\n",
    "def spinning_cursor():\n",
    "    while is_loading:\n",
    "        for cursor in '|/-\\\\':\n",
    "            print(f\"\\r{cursor}\", end=\"\", flush=True)\n",
    "            time.sleep(0.1)\n",
    "    # Clear the spinner line after loading is complete\n",
    "    print(\"\\r\" + \" \" * 20 + \"\\r\", end=\"\", flush=True)\n",
    "\n",
    "is_loading = True\n",
    "spinner_thread = threading.Thread(target=spinning_cursor)\n",
    "spinner_thread.start()\n",
    "\n",
    "df = get_mixpanel_data(FROM_DATE, TO_DATE, force_reload=FORCE_RELOAD_FROM_API)\n",
    "\n",
    "is_loading = False\n",
    "spinner_thread.join()\n",
    "\n",
    "print(f\"\\nTotal de-duplicated events loaded: {len(df)}\")\n",
    "if CLIENT_AGENCY:\n",
    "    mask = df['properties'].apply(lambda p: p.get('client_agency_id') == CLIENT_AGENCY)\n",
    "    df = df[mask]\n",
    "    print(f\"Filtered down to {len(df)} {CLIENT_AGENCY} events\")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abffaed-cd26-4cf3-aefe-48bb84994728",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Analyze data about job holders in buckets by how many jobs they report.\n",
    "'''\n",
    "def get_users_and_accounts_df(df):\n",
    "    # Users who shared the income summary\n",
    "    income_events_df = df[df['event'] == 'ApplicantSharedIncomeSummary'].copy()\n",
    "    \n",
    "    income_events_df['distinct_id'] = income_events_df['properties'].apply(lambda p: p.get('distinct_id'))\n",
    "    income_events_df['timestamp'] = pd.to_datetime(income_events_df['properties'].apply(lambda p: p.get('timestamp')))\n",
    "    income_events_df['account_count'] = income_events_df['properties'].apply(lambda p: p.get('account_count', 0))\n",
    "    income_events_df.sort_values('timestamp', inplace=True)\n",
    "    latest_income_summaries = income_events_df.drop_duplicates(subset=['distinct_id'], keep='last')\n",
    "    account_counts_df = latest_income_summaries.set_index('distinct_id')[['account_count']]\n",
    "    \n",
    "    users_who_shared_pdf = income_events_df['properties'].apply(lambda p: p.get('distinct_id')).unique()\n",
    "    print(f\"Found {len(users_who_shared_pdf)} users with at least one 'ApplicantSharedIncomeSummary' event.\")\n",
    "    mask = df['properties'].apply(lambda p: p.get('distinct_id')).isin(users_who_shared_pdf)\n",
    "    users_who_shared_pdf_df = df[mask]\n",
    "    return users_who_shared_pdf_df, account_counts_df\n",
    "\n",
    "def get_sync_events_df(users_who_shared_pdf_df):\n",
    "    # Find all the Finished*Sync events for these users (these are the \"mega events\")\n",
    "    sync_events = ['ApplicantFinishedArgyleSync', 'ApplicantFinishedPinwheelSync']\n",
    "    sync_events_df = users_who_shared_pdf_df[users_who_shared_pdf_df['event'].isin(sync_events)].copy()\n",
    "    sync_events_df['distinct_id'] = sync_events_df['properties'].apply(lambda p: p.get('distinct_id'))\n",
    "    sync_events_df['timestamp'] = pd.to_datetime(sync_events_df['properties'].apply(lambda p: p.get('timestamp')))\n",
    "    sync_events_df = sync_events_df.sort_values(by=['distinct_id', 'timestamp'])\n",
    "    \n",
    "    # De-duplicate Finished*Sync events based on account id\n",
    "    sync_events_df['account_id'] = sync_events_df['properties'].apply(lambda p: p.get('identity_account_id'))\n",
    "    sync_events_df.dropna(subset=['account_id'], inplace=True)\n",
    "    sync_events_df = sync_events_df.drop_duplicates(\n",
    "        subset=['distinct_id', 'account_id'],\n",
    "        keep='last'\n",
    "    )\n",
    "    \n",
    "    # Extract interesting info from the properties dictionary.\n",
    "    sync_events_df['w2_count'] = sync_events_df['properties'].apply(lambda p: p.get('employment_type_w2_count', 0)).clip(upper=1)\n",
    "    sync_events_df['gig_count'] = sync_events_df['properties'].apply(lambda p: p.get('employment_type_gig_count', 0)).clip(upper=1)\n",
    "\n",
    "    return sync_events_df\n",
    "    \n",
    "users_who_shared_pdf_df, account_counts_df = get_users_and_accounts_df(df)\n",
    "sync_events_df = get_sync_events_df(users_who_shared_pdf_df)\n",
    "\n",
    "# Tabulate the data\n",
    "user_employment_counts = sync_events_df.groupby('distinct_id').agg(\n",
    "    total_w2_count=('w2_count', 'sum'),\n",
    "    total_gig_count=('gig_count', 'sum')\n",
    ")\n",
    "\n",
    "user_employment_counts = pd.merge(\n",
    "    user_employment_counts, \n",
    "    account_counts_df, \n",
    "    on='distinct_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\nJob counts per user: \")\n",
    "user_employment_counts['total_jobs'] = user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count']\n",
    "job_distribution = user_employment_counts['total_jobs'].value_counts().sort_index()\n",
    "print(job_distribution)\n",
    "\n",
    "# Analyze users where the account_count property doesn't match what we expect from the Finished*Sync events\n",
    "# mismatch_mask = user_employment_counts['account_count'] != user_employment_counts['total_jobs']\n",
    "# mismatched_users_df = user_employment_counts[mismatch_mask]\n",
    "# print(f\"\\n{len(mismatched_users_df)} users where account_count does not match total_jobs:\")\n",
    "# print(mismatched_users_df)\n",
    "\n",
    "users_with_gigs_count = len(user_employment_counts[user_employment_counts['total_gig_count'] > 0])\n",
    "print(f\"\\nFound {users_with_gigs_count} users with at least one gig source.\")\n",
    "\n",
    "users_with_one_job = user_employment_counts[\n",
    "    (user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count']) == 1\n",
    "]\n",
    "\n",
    "w2_among_one_job_users = users_with_one_job[users_with_one_job['total_w2_count'] == 1]\n",
    "total_one_job_users = len(users_with_one_job)\n",
    "w2_count = len(w2_among_one_job_users)\n",
    "percentage = (w2_count / total_one_job_users) * 100\n",
    "print(f\"\\nPercentage of single-job users who have a W2 job: {percentage:.2f}%\")\n",
    "\n",
    "median_job_count = user_employment_counts['total_jobs'].median()\n",
    "print(f\"The median job count per user is: {median_job_count}\")\n",
    "\n",
    "multi_job_users = user_employment_counts[\n",
    "    (user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count']) > 1\n",
    "]\n",
    "\n",
    "total_w2_jobs_in_group = multi_job_users['total_w2_count'].sum()\n",
    "total_gig_jobs_in_group = multi_job_users['total_gig_count'].sum()\n",
    "grand_total_jobs_in_group = total_w2_jobs_in_group + total_gig_jobs_in_group\n",
    "percentage = (total_w2_jobs_in_group / grand_total_jobs_in_group) * 100\n",
    "\n",
    "print(f\"\\nFound {len(multi_job_users)} users with more than one job.\")\n",
    "print(f\"Total jobs held by this group: {grand_total_jobs_in_group}\")\n",
    "print(f\"Total W2 jobs held by this group: {total_w2_jobs_in_group}\")\n",
    "print(f\"Among this group, {percentage:.2f}% of all jobs are W2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79bd202-9fb3-4ac4-a448-495aa59140bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging -- See one user's events\n",
    "\n",
    "import pprint\n",
    "\n",
    "target_user_id = 'applicant-110984' \n",
    "temp_df = users_who_shared_pdf_df.copy()\n",
    "\n",
    "# Find all the Finished*Sync events for these users (these are the \"mega events\")\n",
    "sync_events = ['ApplicantFinishedArgyleSync', 'ApplicantFinishedPinwheelSync']\n",
    "temp_df = temp_df[temp_df['event'].isin(sync_events)].copy()\n",
    "temp_df['distinct_id'] = temp_df['properties'].apply(lambda p: p.get('distinct_id'))\n",
    "temp_df['timestamp'] = pd.to_datetime(temp_df['properties'].apply(lambda p: p.get('timestamp')))\n",
    "temp_df = temp_df.sort_values(by=['distinct_id', 'timestamp'])\n",
    "\n",
    "# De-duplicate Finished*Sync events based on account id\n",
    "temp_df['account_id'] = temp_df['properties'].apply(lambda p: p.get('identity_account_id'))\n",
    "temp_df.dropna(subset=['account_id'], inplace=True)\n",
    "temp_df = temp_df.drop_duplicates(\n",
    "    subset=['distinct_id', 'account_id'],\n",
    "    keep='last'\n",
    ")\n",
    "\n",
    "temp_df['distinct_id'] = temp_df['properties'].apply(lambda p: p.get('distinct_id'))\n",
    "temp_df['timestamp'] = pd.to_datetime(\n",
    "    temp_df['properties'].apply(lambda p: p.get('time') or p.get('timestamp')), unit='s'\n",
    ")\n",
    "\n",
    "user_events_df = temp_df[temp_df['distinct_id'] == target_user_id]\n",
    "sorted_user_events = user_events_df.sort_values(by='timestamp')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(f\"Showing all events for user '{target_user_id}', sorted by time:\")\n",
    "print(sorted_user_events[['timestamp', 'event', 'properties']])\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
