{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb7f25b-66a9-43be-8c63-b0bbdc6b425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ------------\n",
    "# INSTRUCTIONS\n",
    "#\n",
    "# Set the FROM_DATE and TO_DATE variables to define the frame of data you want to load.\n",
    "# Then, run this cell. The first time you run it, it will get all the Mixpanel events\n",
    "# in the time range, then it will save that data to a json file in /app/analytics/.\n",
    "# The next time you run this cell, it will load from the json file (unless you change the dates).\n",
    "#\n",
    "# ------------\n",
    "# Helpful Dates\n",
    "#\n",
    "# Completed pilots:\n",
    "#   LA LWC May run:           2025-05-18 - 2025-06-30\n",
    "#   AZ Constrained MAC Pilot: 2025-06-13 - 2025-08-13 7:29pm MST\n",
    "# Incomplete pilots:\n",
    "#   LA LWC August Run:        2025-08-17            - ??? Final day to query before publish date of report\n",
    "#   AZ Expanded MAC Pilot:    2025-08-13 7:30pm MST - ??? Final day to query before publish date of report\n",
    "#\n",
    "# -------------\n",
    "FROM_DATE = '2025-05-18'\n",
    "TO_DATE = '2025-06-30'\n",
    "CLIENT_AGENCY = 'la_ldh'\n",
    "FORCE_RELOAD_FROM_API = False\n",
    "\n",
    "def fetch_mixpanel_data_from_api(params):\n",
    "    API_ENDPOINT = 'https://data.mixpanel.com/api/2.0/export'\n",
    "\n",
    "    try:\n",
    "        project_root = os.path.dirname(os.getcwd())\n",
    "        dotenv_path = os.path.join(project_root, '.env.local')\n",
    "        load_dotenv(dotenv_path=dotenv_path)\n",
    "        \n",
    "        SA_USERNAME = os.environ[\"MIXPANEL_SERVICE_ACCOUNT_USERNAME\"].strip()\n",
    "        SA_SECRET = os.environ[\"MIXPANEL_SERVICE_ACCOUNT_SECRET\"].strip()\n",
    "        PROJECT_ID = os.environ[\"MIXPANEL_PROJECT_ID\"].strip()\n",
    "        AUTH_CREDS = (SA_USERNAME, SA_SECRET)\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Environment variable {e} not found. Please check your .env.local file.\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    params['project_id'] = PROJECT_ID\n",
    "    \n",
    "    print(\"Fetching data from Mixpanel API...\")\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            API_ENDPOINT,\n",
    "            headers={\"accept\": \"text/plain\"},\n",
    "            params=params,\n",
    "            auth=AUTH_CREDS\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        print(\"Successfully fetched data from API.\")\n",
    "        return response.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error: API request failed. {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "def parse_raw_data_to_df(raw_text):\n",
    "    \"\"\"Parses newline-delimited JSON text into a pandas DataFrame.\"\"\"\n",
    "    raw_data = []\n",
    "    for line in raw_text.strip().split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            raw_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not decode line: {line}\", file=sys.stderr)\n",
    "    \n",
    "    if not raw_data:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    return pd.DataFrame(raw_data)\n",
    "\n",
    "def deduplicate_events(df):\n",
    "    \"\"\"De-duplicates a DataFrame of Mixpanel events, keeping the latest event per $insert_id.\"\"\"\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    print(f\"Original event count: {len(df)}\")\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(\n",
    "        df['properties'].apply(lambda p: p.get('time') or p.get('timestamp')),\n",
    "        unit='s'\n",
    "    )\n",
    "    df['$insert_id'] = df['properties'].apply(lambda p: p.get('$insert_id'))\n",
    "    \n",
    "    # Drop rows where '$insert_id' is missing, as they cannot be de-duplicated\n",
    "    df.dropna(subset=['$insert_id'], inplace=True)\n",
    "    \n",
    "    df.sort_values('timestamp', inplace=True)\n",
    "    df.drop_duplicates(subset=['$insert_id'], keep='last', inplace=True)\n",
    "    df.drop(columns=['$insert_id'], inplace=True)\n",
    "    \n",
    "    print(f\"Event count after de-duplication: {len(df)}\")\n",
    "    return df\n",
    "\n",
    "def get_mixpanel_data(from_date, to_date, force_reload=False):\n",
    "    \"\"\"\n",
    "    Loads de-duplicated Mixpanel data from a local file if it exists,\n",
    "    otherwise fetches, processes, de-duplicates, and saves it.\n",
    "    \"\"\"\n",
    "    # NOTE: The saved file is now the CLEAN, DE-DUPLICATED data.\n",
    "    file_name = f\"mixpanel_data_{from_date}_to_{to_date}.json\"\n",
    "    \n",
    "    if os.path.exists(file_name) and not force_reload:\n",
    "        print(f\"Loading de-duplicated data from local file: {file_name}\")\n",
    "        clean_df = pd.read_json(file_name, orient='records', lines=True)\n",
    "        return clean_df\n",
    "\n",
    "    # Fetch data from API\n",
    "    params = {'from_date': from_date, 'to_date': to_date}\n",
    "    raw_text = fetch_mixpanel_data_from_api(params)\n",
    "    \n",
    "    if raw_text is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # De-duplicate\n",
    "    df = parse_raw_data_to_df(raw_text)\n",
    "    clean_df = deduplicate_events(df)\n",
    "    \n",
    "    # Save the de-duplicated data for future use\n",
    "    if not clean_df.empty:\n",
    "        clean_df.to_json(file_name, orient='records', lines=True, date_format='iso')\n",
    "        print(f\"Clean, de-duplicated data saved to {file_name}\")\n",
    "        \n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def spinning_cursor():\n",
    "    while is_loading:\n",
    "        for cursor in '|/-\\\\':\n",
    "            print(f\"\\r{cursor}\", end=\"\", flush=True)\n",
    "            time.sleep(0.1)\n",
    "    # Clear the spinner line after loading is complete\n",
    "    print(\"\\r\" + \" \" * 20 + \"\\r\", end=\"\", flush=True)\n",
    "\n",
    "is_loading = True\n",
    "spinner_thread = threading.Thread(target=spinning_cursor)\n",
    "spinner_thread.start()\n",
    "\n",
    "df = get_mixpanel_data(FROM_DATE, TO_DATE, force_reload=FORCE_RELOAD_FROM_API)\n",
    "\n",
    "is_loading = False\n",
    "spinner_thread.join()\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"\\nTotal deduplicated events loaded: {len(df)}\")\n",
    "    if CLIENT_AGENCY:\n",
    "        mask = df['properties'].apply(lambda p: p.get('client_agency_id') == CLIENT_AGENCY)\n",
    "        df_filtered = df[mask]\n",
    "        print(f\"Found {len(df_filtered)} events with client_agency_id set to '{CLIENT_AGENCY}'\")\n",
    "else:\n",
    "    print(\"No data available to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abffaed-cd26-4cf3-aefe-48bb84994728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users who shared the income summary\n",
    "income_events_df = df[df['event'] == 'ApplicantSharedIncomeSummary']\n",
    "users_who_shared_pdf = income_events_df['properties'].apply(lambda p: p.get('distinct_id')).unique()\n",
    "print(f\"Found {len(users_who_shared_pdf)} users with at least one 'ApplicantSharedIncomeSummary' event.\")\n",
    "\n",
    "# Create a df of just these users\n",
    "mask = df['properties'].apply(lambda p: p.get('distinct_id')).isin(users_who_shared_pdf)\n",
    "users_who_shared_pdf_df = df[mask]\n",
    "\n",
    "# Find all the Finished*Sync events for these users (these are the \"mega events\")\n",
    "sync_events = ['ApplicantFinishedArgyleSync', 'ApplicantFinishedPinwheelSync']\n",
    "sync_events_df = users_who_shared_pdf_df[users_who_shared_pdf_df['event'].isin(sync_events)].copy()\n",
    "\n",
    "# Extract the distinct_id and timestamp from the properties dictionary.\n",
    "sync_events_df['distinct_id'] = sync_events_df['properties'].apply(lambda p: p.get('distinct_id'))\n",
    "sync_events_df['timestamp'] = pd.to_datetime(sync_events_df['properties'].apply(lambda p: p.get('timestamp')))\n",
    "\n",
    "# Sort the DataFrame by user and then by time to ensure the most recent event is last\n",
    "sync_events_df = sync_events_df.sort_values(by=['distinct_id', 'timestamp'])\n",
    "\n",
    "# De-duplicate and keep only the most recent event for each user\n",
    "latest_sync_per_user_df = sync_events_df.drop_duplicates(subset='distinct_id', keep='last').copy()\n",
    "\n",
    "# Extract the final counts from this de-duplicated DataFrame\n",
    "latest_sync_per_user_df['total_w2_count'] = latest_sync_per_user_df['properties'].apply(lambda p: p.get('employment_type_w2_count', 0))\n",
    "latest_sync_per_user_df['total_gig_count'] = latest_sync_per_user_df['properties'].apply(lambda p: p.get('employment_type_gig_count', 0))\n",
    "\n",
    "# Create a DataFrame with the user as the index\n",
    "user_employment_counts = latest_sync_per_user_df.set_index('distinct_id')[['total_w2_count', 'total_gig_count']]\n",
    "\n",
    "# Df of users who have at least one job\n",
    "users_with_jobs_df = user_employment_counts[\n",
    "    (user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count'] > 0)\n",
    "]\n",
    "\n",
    "print(\"\\nJob counts per user: \")\n",
    "user_employment_counts['total_jobs'] = user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count']\n",
    "job_distribution = user_employment_counts['total_jobs'].value_counts().sort_index()\n",
    "print(job_distribution)\n",
    "\n",
    "users_with_gigs_count = len(user_employment_counts[user_employment_counts['total_gig_count'] > 0])\n",
    "print(f\"\\nFound {users_with_gigs_count} users with at least one gig source.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79bd202-9fb3-4ac4-a448-495aa59140bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging -- See one user's events\n",
    "\n",
    "import pprint\n",
    "\n",
    "target_user_id = 'applicant-100114' \n",
    "temp_df = users_who_shared_pdf_df.copy()\n",
    "temp_df['distinct_id'] = temp_df['properties'].apply(lambda p: p.get('distinct_id'))\n",
    "temp_df['timestamp'] = pd.to_datetime(\n",
    "    temp_df['properties'].apply(lambda p: p.get('time') or p.get('timestamp')), unit='s'\n",
    ")\n",
    "\n",
    "user_events_df = temp_df[temp_df['distinct_id'] == target_user_id]\n",
    "sorted_user_events = user_events_df.sort_values(by='timestamp')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(f\"Showing all events for user '{target_user_id}', sorted by time:\")\n",
    "print(sorted_user_events[['timestamp', 'event', 'properties']])\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6519c4-8510-4104-87d7-b367ec39ee33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
