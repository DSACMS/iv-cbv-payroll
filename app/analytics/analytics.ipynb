{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e576f097-1ccf-4a99-9815-0c704a2d04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from mixpanel import Mixpanel\n",
    "\n",
    "# ------------\n",
    "# INSTRUCTIONS\n",
    "#\n",
    "# Set the from_date and to_date variables to define the frame of data you want to load.\n",
    "# Then, run this cell. The first time you run it, it will get all the Mixpanel events\n",
    "# in the time range, then it will save that data to a json file in /app/analytics/.\n",
    "# The next time you run this cell, it will load from the json file (unless you change the dates).\n",
    "#\n",
    "# ------------\n",
    "# Helpful Dates\n",
    "#\n",
    "# Completed pilots:\n",
    "#   LA LWC May run:           2025-05-18 - 2025-06-30\n",
    "#   AZ Constrained MAC Pilot: 2025-06-13 - 2025-08-13 7:29pm MST\n",
    "# Incomplete pilots:\n",
    "#   LA LWC August Run:        2025-08-17            - ??? Final day to query before publish date of report\n",
    "#   AZ Expanded MAC Pilot:    2025-08-13 7:30pm MST - ??? Final day to query before publish date of report\n",
    "#\n",
    "# -------------\n",
    "\n",
    "from_date = '2025-05-18'\n",
    "to_date = '2025-06-30'\n",
    "client_agency = 'la_ldh'\n",
    "\n",
    "# Credentials\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "dotenv_path = os.path.join(project_root, '.env.local')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "SA_USERNAME = os.getenv(\"MIXPANEL_SERVICE_ACCOUNT_USERNAME\").strip()\n",
    "SA_SECRET = os.getenv(\"MIXPANEL_SERVICE_ACCOUNT_SECRET\").strip()\n",
    "PROJECT_ID = os.getenv(\"MIXPANEL_PROJECT_ID\").strip()\n",
    "\n",
    "# Endpoint\n",
    "API_ENDPOINT = 'https://data.mixpanel.com/api/2.0/export'\n",
    "headers = {\"accept\": \"text/plain\"}\n",
    "\n",
    "# Date range\n",
    "file_name = f\"mixpanel_data_{from_date}_to_{to_date}.json\"\n",
    "params = {\n",
    "    'from_date': from_date,\n",
    "    'to_date': to_date,\n",
    "    'project_id': PROJECT_ID\n",
    "}\n",
    "\n",
    "# Loading animation setup\n",
    "is_loading = False\n",
    "\n",
    "def spinning_cursor():\n",
    "    while is_loading:\n",
    "        for cursor in '|/-\\\\':\n",
    "            print(f\"\\r{cursor}\", end=\"\", flush=True)\n",
    "            time.sleep(0.1)\n",
    "    # Clear the spinner line after loading is complete\n",
    "    print(\"\\r\" + \" \" * 20 + \"\\r\", end=\"\", flush=True)\n",
    "\n",
    "# Check if the data has already been downloaded\n",
    "if os.path.exists(file_name):\n",
    "    print(f\"Loading data from local file: {file_name}\")\n",
    "    is_loading = True\n",
    "    spinner_thread = threading.Thread(target=spinning_cursor)\n",
    "    spinner_thread.start()\n",
    "    \n",
    "    with open(file_name, 'r') as f:\n",
    "        raw_response_text = f.read()\n",
    "    \n",
    "    is_loading = False\n",
    "    spinner_thread.join()\n",
    "else:\n",
    "    is_loading = True\n",
    "    spinner_thread = threading.Thread(target=spinning_cursor)\n",
    "    spinner_thread.start()\n",
    "    \n",
    "    try:\n",
    "        # Make the request\n",
    "        print(\"Fetching data from Mixpanel...\")\n",
    "        response = requests.get(\n",
    "            API_ENDPOINT,\n",
    "            headers=headers,\n",
    "            params=params,\n",
    "            auth=(SA_USERNAME, SA_SECRET)\n",
    "        )\n",
    "        \n",
    "        is_loading = False\n",
    "        spinner_thread.join()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"Successfully fetched data from API.\")\n",
    "            raw_response_text = response.text\n",
    "            # Save the raw response to the file for future use\n",
    "            with open(file_name, 'w') as f:\n",
    "                f.write(raw_response_text)\n",
    "            print(f\"Data saved to {file_name}\")\n",
    "        else:\n",
    "            print(f\"Error: API request failed with status code {response.status_code}\")\n",
    "            print(response.text)\n",
    "            exit()\n",
    "    except:\n",
    "        is_loading = False\n",
    "        spinner_thread.join()\n",
    "        print(f\"\\nAn error occurred during the API request: {e}\")\n",
    "        exit()\n",
    "\n",
    "# Process the data (either from the file or the fresh API call)\n",
    "if raw_response_text:\n",
    "    raw_data = []\n",
    "    lines = raw_response_text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            raw_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not decode line: {line}\")\n",
    "\n",
    "    if raw_data:\n",
    "        df = pd.DataFrame(raw_data)\n",
    "        print(f\"Successfully processed {len(df)} events.\")\n",
    "    else:\n",
    "        print(\"No data was processed.\")\n",
    "else:\n",
    "    print(\"No data available to process.\")\n",
    "\n",
    "if len(client_agency) > 0:\n",
    "    # Filter down to events from a specific state pilot\n",
    "    mask = df['properties'].apply(lambda p: p.get('client_agency_id') == client_agency)\n",
    "    df = df[mask]\n",
    "    \n",
    "    print(f\"Found {len(df)} events with client_agency_id set to\", client_agency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abffaed-cd26-4cf3-aefe-48bb84994728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users who shared the income summary\n",
    "income_events_df = df[df['event'] == 'ApplicantSharedIncomeSummary']\n",
    "users_who_shared_pdf = income_events_df['properties'].apply(lambda p: p.get('distinct_id')).unique()\n",
    "print(f\"Found {len(users_who_shared_pdf)} users with at least one 'ApplicantSharedIncomeSummary' event.\")\n",
    "\n",
    "# Create a df of just these users\n",
    "mask = df['properties'].apply(lambda p: p.get('distinct_id')).isin(users_who_shared_pdf)\n",
    "users_who_shared_pdf_df = df[mask]\n",
    "\n",
    "# Find all the Finished*Sync events for these users (these are the \"mega events\")\n",
    "sync_events = ['ApplicantFinishedArgyleSync', 'ApplicantFinishedPinwheelSync']\n",
    "sync_events_df = users_who_shared_pdf_df[users_who_shared_pdf_df['event'].isin(sync_events)].copy()\n",
    "\n",
    "sync_events_df['distinct_id'] = sync_events_df['properties'].apply(lambda p: p.get('distinct_id'))\n",
    "sync_events_df['w2_count'] = sync_events_df['properties'].apply(lambda p: p.get('employment_type_w2_count', 0))\n",
    "sync_events_df['gig_count'] = sync_events_df['properties'].apply(lambda p: p.get('employment_type_gig_count', 0))\n",
    "\n",
    "# Group by the user and calculate the sum of jobs reported\n",
    "user_employment_counts = sync_events_df.groupby('distinct_id').agg(\n",
    "    total_w2_count=('w2_count', 'sum'),\n",
    "    total_gig_count=('gig_count', 'sum')\n",
    ")\n",
    "\n",
    "# Df of users who have at least one job\n",
    "users_with_jobs_df = user_employment_counts[\n",
    "    ((user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count'] > 0))\n",
    "]\n",
    "\n",
    "print(\"Total W2 and Gig counts per user: \")\n",
    "print(user_employment_counts)\n",
    "print(f\"\\nFound {len(users_with_jobs_df)} users with at least one W2 or gig source.\")\n",
    "print(users_with_jobs_df.head())\n",
    "\n",
    "# Df of users who have exactly one job\n",
    "users_with_one_job_df = user_employment_counts[\n",
    "    ((user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count'] == 1))\n",
    "]\n",
    "\n",
    "print(f\"\\nFound {len(users_with_one_job_df)} users with exactly one job.\")\n",
    "print(users_with_one_job_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e092d9d-b850-4bec-b51d-ddfca1e65967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, print out the matching events for a single user\n",
    "specific_user_id = \"applicant-99999\"\n",
    "print(f\"--- Investigating all sync events for user: {specific_user_id} ---\")\n",
    "single_user_sync_events = sync_events_df[sync_events_df['distinct_id'] == specific_user_id]\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 100, 'display.max_colwidth', 100):\n",
    "    print(single_user_sync_events[['event', 'properties', 'w2_count', 'gig_count']].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ecb9f-668d-4361-8021-7eee762937a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
