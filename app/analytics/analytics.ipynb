{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5110486e-75b6-44b5-b22c-6e9d74569bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import threading\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from mixpanel import Mixpanel\n",
    "\n",
    "# ------------\n",
    "# INSTRUCTIONS\n",
    "#\n",
    "# Set the from_date and to_date variables to define the frame of data you want to load.\n",
    "# Then, run this cell. The first time you run it, it will get all the Mixpanel events\n",
    "# in the time range, then it will save that data to a json file in /app/analytics/.\n",
    "# The next time you run this cell, it will load from the json file (unless you change the dates).\n",
    "#\n",
    "# ------------\n",
    "# Helpful Dates\n",
    "#\n",
    "# Completed pilots:\n",
    "#   LA LWC May run:           2025-05-18 - 2025-06-30\n",
    "#   AZ Constrained MAC Pilot: 2025-06-13 - 2025-08-13 7:29pm MST\n",
    "# Incomplete pilots:\n",
    "#   LA LWC August Run:        2025-08-17            - ??? Final day to query before publish date of report\n",
    "#   AZ Expanded MAC Pilot:    2025-08-13 7:30pm MST - ??? Final day to query before publish date of report\n",
    "#\n",
    "# -------------\n",
    "\n",
    "from_date = '2025-05-18'\n",
    "to_date = '2025-06-30'\n",
    "client_agency = 'la_ldh'\n",
    "\n",
    "# Credentials\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "dotenv_path = os.path.join(project_root, '.env.local')\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "SA_USERNAME = os.getenv(\"MIXPANEL_SERVICE_ACCOUNT_USERNAME\").strip()\n",
    "SA_SECRET = os.getenv(\"MIXPANEL_SERVICE_ACCOUNT_SECRET\").strip()\n",
    "PROJECT_ID = os.getenv(\"MIXPANEL_PROJECT_ID\").strip()\n",
    "\n",
    "# Endpoint\n",
    "API_ENDPOINT = 'https://data.mixpanel.com/api/2.0/export'\n",
    "headers = {\"accept\": \"text/plain\"}\n",
    "\n",
    "# Date range\n",
    "file_name = f\"mixpanel_data_{from_date}_to_{to_date}.json\"\n",
    "params = {\n",
    "    'from_date': from_date,\n",
    "    'to_date': to_date,\n",
    "    'project_id': PROJECT_ID\n",
    "}\n",
    "\n",
    "# Loading animation setup\n",
    "is_loading = False\n",
    "\n",
    "def spinning_cursor():\n",
    "    while is_loading:\n",
    "        for cursor in '|/-\\\\':\n",
    "            print(f\"\\r{cursor}\", end=\"\", flush=True)\n",
    "            time.sleep(0.1)\n",
    "    # Clear the spinner line after loading is complete\n",
    "    print(\"\\r\" + \" \" * 20 + \"\\r\", end=\"\", flush=True)\n",
    "\n",
    "# Check if the data has already been downloaded\n",
    "if os.path.exists(file_name):\n",
    "    print(f\"Loading data from local file: {file_name}\")\n",
    "    is_loading = True\n",
    "    spinner_thread = threading.Thread(target=spinning_cursor)\n",
    "    spinner_thread.start()\n",
    "    \n",
    "    with open(file_name, 'r') as f:\n",
    "        raw_response_text = f.read()\n",
    "    \n",
    "    is_loading = False\n",
    "    spinner_thread.join()\n",
    "else:\n",
    "    is_loading = True\n",
    "    spinner_thread = threading.Thread(target=spinning_cursor)\n",
    "    spinner_thread.start()\n",
    "    \n",
    "    try:\n",
    "        # Make the request\n",
    "        print(\"Fetching data from Mixpanel...\")\n",
    "        response = requests.get(\n",
    "            API_ENDPOINT,\n",
    "            headers=headers,\n",
    "            params=params,\n",
    "            auth=(SA_USERNAME, SA_SECRET)\n",
    "        )\n",
    "        \n",
    "        is_loading = False\n",
    "        spinner_thread.join()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"Successfully fetched data from API.\")\n",
    "            raw_response_text = response.text\n",
    "            # Save the raw response to the file for future use\n",
    "            with open(file_name, 'w') as f:\n",
    "                f.write(raw_response_text)\n",
    "            print(f\"Data saved to {file_name}\")\n",
    "        else:\n",
    "            print(f\"Error: API request failed with status code {response.status_code}\")\n",
    "            print(response.text)\n",
    "            exit()\n",
    "    except:\n",
    "        is_loading = False\n",
    "        spinner_thread.join()\n",
    "        print(f\"\\nAn error occurred during the API request: {e}\")\n",
    "        exit()\n",
    "\n",
    "# Process the data (either from the file or the fresh API call)\n",
    "if raw_response_text:\n",
    "    raw_data = []\n",
    "    lines = raw_response_text.strip().split('\\n')\n",
    "    for line in lines:\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            raw_data.append(json.loads(line))\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not decode line: {line}\")\n",
    "\n",
    "    if raw_data:\n",
    "        df = pd.DataFrame(raw_data)\n",
    "        print(f\"Successfully processed {len(df)} events.\")\n",
    "    else:\n",
    "        print(\"No data was processed.\")\n",
    "else:\n",
    "    print(\"No data available to process.\")\n",
    "\n",
    "if len(client_agency) > 0:\n",
    "    # Filter down to events from a specific state pilot\n",
    "    mask = df['properties'].apply(lambda p: p.get('client_agency_id') == client_agency)\n",
    "    df = df[mask]\n",
    "    \n",
    "    print(f\"Found {len(df)} events with client_agency_id set to\", client_agency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abffaed-cd26-4cf3-aefe-48bb84994728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Users who shared the income summary\n",
    "income_events_df = df[df['event'] == 'ApplicantSharedIncomeSummary']\n",
    "users_who_shared_pdf = income_events_df['properties'].apply(lambda p: p.get('distinct_id')).unique()\n",
    "print(f\"Found {len(users_who_shared_pdf)} users with at least one 'ApplicantSharedIncomeSummary' event.\")\n",
    "\n",
    "# Create a df of just these users\n",
    "mask = df['properties'].apply(lambda p: p.get('distinct_id')).isin(users_who_shared_pdf)\n",
    "users_who_shared_pdf_df = df[mask]\n",
    "\n",
    "# Find all the Finished*Sync events for these users (these are the \"mega events\")\n",
    "sync_events = ['ApplicantFinishedArgyleSync', 'ApplicantFinishedPinwheelSync']\n",
    "sync_events_df = users_who_shared_pdf_df[users_who_shared_pdf_df['event'].isin(sync_events)].copy()\n",
    "\n",
    "# Extract the distinct_id and timestamp from the properties dictionary.\n",
    "sync_events_df['distinct_id'] = sync_events_df['properties'].apply(lambda p: p.get('distinct_id'))\n",
    "sync_events_df['timestamp'] = pd.to_datetime(sync_events_df['properties'].apply(lambda p: p.get('timestamp')))\n",
    "\n",
    "# Sort the DataFrame by user and then by time to ensure the most recent event is last\n",
    "sync_events_df = sync_events_df.sort_values(by=['distinct_id', 'timestamp'])\n",
    "\n",
    "# De-duplicate and keep only the most recent event for each user\n",
    "latest_sync_per_user_df = sync_events_df.drop_duplicates(subset='distinct_id', keep='last').copy()\n",
    "\n",
    "# Extract the final counts from this de-duplicated DataFrame\n",
    "latest_sync_per_user_df['total_w2_count'] = latest_sync_per_user_df['properties'].apply(lambda p: p.get('employment_type_w2_count', 0))\n",
    "latest_sync_per_user_df['total_gig_count'] = latest_sync_per_user_df['properties'].apply(lambda p: p.get('employment_type_gig_count', 0))\n",
    "\n",
    "# Create a DataFrame with the user as the index\n",
    "user_employment_counts = latest_sync_per_user_df.set_index('distinct_id')[['total_w2_count', 'total_gig_count']]\n",
    "\n",
    "# Df of users who have at least one job\n",
    "users_with_jobs_df = user_employment_counts[\n",
    "    (user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count'] > 0)\n",
    "]\n",
    "\n",
    "print(\"\\nW2 and Gig counts per user: \")\n",
    "print(user_employment_counts)\n",
    "print(f\"\\nFound {len(users_with_jobs_df)} users with at least one W2 or gig source.\")\n",
    "print(users_with_jobs_df.head())\n",
    "\n",
    "users_with_one_job_df = user_employment_counts[\n",
    "    ((user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count'] == 1))\n",
    "]\n",
    "print(f\"\\nFound {len(users_with_one_job_df)} users with exactly one job.\")\n",
    "\n",
    "users_with_two_jobs_df = user_employment_counts[\n",
    "    ((user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count'] == 2))\n",
    "]\n",
    "print(f\"\\nFound {len(users_with_two_jobs_df)} users with exactly two jobs.\")\n",
    "\n",
    "users_with_three_jobs_df = user_employment_counts[\n",
    "    ((user_employment_counts['total_w2_count'] + user_employment_counts['total_gig_count'] == 3))\n",
    "]\n",
    "print(f\"\\nFound {len(users_with_three_jobs_df)} users with exactly three jobs.\")\n",
    "\n",
    "users_with_gigs_df = user_employment_counts[\n",
    "    (user_employment_counts['total_gig_count'] > 0)\n",
    "]\n",
    "print(f\"\\nFound {len(users_with_gigs_df)} users with gigs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e092d9d-b850-4bec-b51d-ddfca1e65967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For debugging, print out the matching events for a single user\n",
    "specific_user_id = \"applicant-62230\"\n",
    "print(f\"--- Investigating all sync events for user: {specific_user_id} ---\")\n",
    "single_user_sync_events = sync_events_df[sync_events_df['distinct_id'] == specific_user_id]\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 100, 'display.max_colwidth', 100):\n",
    "    print(single_user_sync_events[['event', 'properties']].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0ecb9f-668d-4361-8021-7eee762937a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many shared events does each user have?\n",
    "# Extract the distinct_id for each of these events\n",
    "event_user_ids = income_events_df['properties'].apply(lambda p: p.get('distinct_id'))\n",
    "\n",
    "# Count the occurrences of each user ID\n",
    "user_event_counts = event_user_ids.value_counts()\n",
    "\n",
    "# Filter this list to find users who appear more than once\n",
    "users_with_multiple_shares = user_event_counts[user_event_counts > 1]\n",
    "users_with_one_share = user_event_counts[user_event_counts == 1]\n",
    "users_with_many_shares = user_event_counts[user_event_counts > 3]\n",
    "\n",
    "print(users_with_one_share)\n",
    "print(users_with_multiple_shares)\n",
    "print(users_with_many_shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79bd202-9fb3-4ac4-a448-495aa59140bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# See one user's events\n",
    "target_user_id = 'applicant-100114' \n",
    "temp_df = users_who_shared_pdf_df.copy()\n",
    "temp_df['distinct_id'] = temp_df['properties'].apply(lambda p: p.get('distinct_id'))\n",
    "temp_df['timestamp'] = pd.to_datetime(\n",
    "    temp_df['properties'].apply(lambda p: p.get('time') or p.get('timestamp'))\n",
    ")\n",
    "\n",
    "user_events_df = temp_df[temp_df['distinct_id'] == target_user_id]\n",
    "sorted_user_events = user_events_df.sort_values(by='timestamp')\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(f\"Showing all events for user '{target_user_id}', sorted by time:\")\n",
    "print(sorted_user_events[['timestamp', 'event', 'properties']])\n",
    "# row_index = 382646\n",
    "# properties_data = temp_df.loc[row_index, 'properties']\n",
    "\n",
    "# # Use pprint to print it in a nicely formatted way\n",
    "# print(f\"Properties for row {row_index}:\")\n",
    "# pprint.pprint(properties_data)\n",
    "\n",
    "pd.reset_option('display.max_rows')\n",
    "pd.reset_option('display.max_colwidth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6519c4-8510-4104-87d7-b367ec39ee33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
