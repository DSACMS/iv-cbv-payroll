---
title: "EMMY Mixpanel Web Analytic Review"
execute:
  echo: false
  output: false
  warning: false
  #cache: true
format:
  html:
    toc: true
    number-sections: true
    css: dsac-theme.css
    embed-resources: true
  # docx:
  #   toc: true
  #   number-sections: true
  #   reference-doc: custom-reference-doc.docx
  #   highlight-style: github
  #   dpi: 300
---

# Purpose

Over the past year, the Centers for Medicare and Medicaid Services (CMS) in conjunction with partner states have piloted an online income verification application, Eligibility Made Easy (EMMY). With each phase, analysts have explore the web analytics data, looking into applicant uptake and utilization of the tool. This specific report aims identify areas of the tool where retention drops in an effort to improve the product design, making it easier for applicants to complete the full form.

## Key Questions

Below are the areas the report seeks to explore to help improve the product design.

1.  How many users are starting but not completing the process?
2.  Where in the process are applicants dropping off?
3.  Does time appear to be a factor in dropping off?
    1.  How long does the data syncing with Argyle/Pinwheel take?
4.  Are applicants accessing the documentation (help or FAQ) before dropping off?
5.  How many applicants are not finding their employer in the search?
6.  Are the popular app employers the right ones (i.e. do there appear to be employeers searched more frequently than any that appear in the popular links)
7.  What share of applicants are return users? (tokenized users)
    1.  How many sessions does it take an applicants to complete the process?
    2.  How many applicants clicked on the link and then came back another time to complete the process?
8.  Do we see a difference in completion rates if applicants are accessing from mobile vs desktop?
9.  How many times does a user "browser back"?

-   Possible Comparisions
    -   device types
    -   pilot periods/weeks

## Method

This report was generated using a Quarto notebook and leverage R for all analysis and visualizations. The analysis focuses on the 2025 Lousiana pilot, which had three, 6-week session starting May, August, and November. The web analytics data come from Mixpanel, accessed using python for the API call with a [Jupyter Notebook](https://github.com/DSACMS/iv-cbv-payroll/blob/main/app/analytics/analytics.ipynb). All of the underlying code can be found on [GitHub](https://github.com/DSACMS/iv-cbv-payroll/tree/main/app/analytics).

## Defintions/Calculations

Its easy to construct similar metrics in different ways and yield slightly different results. For tranparency, below are some definitions to key concepts or calculations

-   **Applicant**: uses the `distinct_id` associated with the event.
-   **Session**: relies on the `properties$cbv_flow_id`. A user may have one or more sessions during a pilot period.
-   **Total Event**: When an applicant has multiple sessions, each of these unique applicant and sessions is called an total event in Mixpanel.
-   **Application submitted/successful**: the applicant finished the submission flow ending on the success page (`event == 'ApplicantSharedIncomeSummary'`). 
-   **Application start time**: uses the `timestamp` the applicant viewed the agreement page (`event == 'ApplicantViewedAgreement'`)
-   **Application duration**: the duration is then calculated from the difference between the session start time and the `timestamp` the last event was viewed.
-  **Provider sync time**: calculated using the `timestamp` between two events, `ApplicantSucceededWithLogin` and `ApplicantFinishedSync`

# Analysis

```{r}
#| label: dependencies

#dependencies to load
library(tidyverse)
library(here)
library(arrow, warn.conflicts = FALSE)
library(jsonlite, warn.conflicts = FALSE)
library(glue)
library(patchwork)
library(scales)
library(gt)
library(ggtext)
library(ggsankey) ## remotes::install_github("davidsjoberg/ggsankey")
library(glamr) ##install.packages('glamr', repos = c('https://usaid-oha-si.r-universe.dev', 'https://cloud.r-project.org'))
library(glitr) #install.packages('glitr', repos = c('https://usaid-oha-si.r-universe.dev', 'https://cloud.r-project.org'))
library(gtayblr) ## remotes::install_github("USAID-OHA-SI/gtayblr")
```

```{r}
#| label: global variables
#| echo: false
#| warning: false

#identify paths for json data for each of the three periods
mp_path <- list.files(here("app/analytics"), "mixpanel.*parquet", full.names = TRUE)

#default viz/table caption
default_caption <- "Source: Lousiana 2025 EMMY Pilot Mixpanel Data [accessed 2025-12-15]"

#set DSAC colors
dsac_color <- c("#103D68", "#136A5D", "#6A1344", "#EFAC2F", "#63789D", "#C1C9D7", "#5A9088", "#D9E8E5", "#842F66", "#123054")
dsac_color_name <- c("navy", "teal", "cranberry", "gold", "light_navy", "pale_navy", "light_teal", "pale_teal", "light_cranberry", "dark_navy")
dsac_color <- setNames(dsac_color, dsac_color_name)
rm(dsac_color_name)
```

```{r}
#| label: import_data

df_mp <- read_parquet(mp_path)
```

```{r}
#| label: fcn_clean_events

#clean event names for viz to add space and remove repetitive "applicant" text
clean_events <- function(df){

  df |> 
    mutate(
      event_clean = event |> 
        str_replace_all("(?<!^)([A-Z])", " \\1") |> 
        str_remove("Applicant ") |> 
        str_remove(" Or Platform Item"),
        .after = event
        )
}
```

## How many users are starting but not completing the process?

To start off with, we need to understand how many applicants are starting out this process. The first pilot in May did not have the same means of determining unique applicants, so the table below only reports the total events for May. August saw the largest number of applicants start the process(i.e. viewed the agreement), but November had the highest completion rate. 

```{r}
#| label: retention_tbl
#| output: true

 df_compl_pilot <- df_mp |> 
  filter(event %in% c("ApplicantViewedAgreement","ApplicantSharedIncomeSummary")) |> 
  clean_events() |> 
  group_by(pilot, event_clean) |> 
  summarise(
    applicants = n_distinct(distinct_id),
    events = n(),
    .groups = "drop"
    ) |>
  pivot_longer(c(applicants, events), names_to = "denom") |> 
  pivot_wider(
    names_from = event_clean,
    values_from = value
    ) |> 
  mutate(`Completion Rate` = `Shared Income Summary` / `Viewed Agreement`) |> 
  mutate(across(where(is.numeric), ~ ifelse(pilot == "May 2025" & denom == "applicants", NA_integer_, .x)))

df_compl_pilot |> 
  gt(groupname_col = "denom") |> 
  cols_move(`Viewed Agreement`, after = pilot) |> 
  format_numeric_columns() |> 
  si_gt_base(header_fill = dsac_color['light_teal']) |> 
  adjust_row_padding() |> 
  add_labs(
    title = "STEADY COMPLETION RATE IN ALL PILOTS",
    subtitle = "The distinct number of applicants who started and completed the process",
    caption = default_caption
  ) |> 
    fmt_missing()

```

```{r}
#| label: multi_complete

df_success_diff <- df_mp |> 
  filter(
    event %in% c("ApplicantSharedIncomeSummary"),
    pilot == "Nov 2025"
    ) |> 
  group_by(pilot) |>
  summarize(
    applicants = n_distinct(distinct_id),
    sessions = n_distinct(distinct_id, cbv_flow_id),
    .groups = 'drop'
  )
```
There are a number of applicants who had multiple successful submissions (across multiple sessions, which). In the November pilot, for example, there were `{r} scales::label_comma()(df_success_diff$applicants)` distinct applicants who submitted compared with `{r} scales::label_comma()(df_success_diff$sessions)` total events.  

```{r}
#| label: multi_complete_tbl_calc

df_multi_tbl <- df_mp |> 
  filter(event %in% c("ApplicantSharedIncomeSummary")) |> 
  distinct(pilot, distinct_id, cbv_flow_id) |> 
  count(pilot, distinct_id, sort = TRUE, name = "submissions") |> 
  count(pilot, submissions, name = "applicants") |> 
  arrange(submissions) |> 
  group_by(pilot) |> 
  mutate(
    `Total Applicants` = sum(applicants),
    `Total Events` = sum(submissions*applicants)) |> 
  ungroup() 

#November data for text
df_multi_tbl_nov <- df_multi_tbl |> 
  filter(pilot == "Nov 2025")
```

The table below details how many submissions were associated with an applicant.^[Since not all applicants are using tokenized link, there may in fact be more applicants successfully submitting multiple times] In the November case, there were `r sum(df_multi_tbl_nov[df_multi_tbl_nov$submissions > 1, ]$applicants)` applicants who submitted more than one time; most of those multi-submission applicants, `r sum(df_multi_tbl_nov[df_multi_tbl_nov$submissions == 2, ]$applicants)`, submitted their applicants twice.   

```{r}
#| label: multi_complete_tbl
#| output: true
#| 
df_multi_tbl |> 
  pivot_wider(names_from = submissions, values_from = applicants) |> 
  gt() |>
  format_numeric_columns() |> 
  sub_missing() |> 
  si_gt_base(header_fill = dsac_color['light_teal']) |> 
  tab_spanner(
    label = "# of submissions",
    columns = -c(pilot, `Total Applicants`, `Total Events`)
  ) |> 
  adjust_row_padding() |> 
  add_labs(
    title = "Some users had multiple successful submissions events" |> toupper(),
    subtitle = "the number of successful submission",
    caption = glue("Note: total sessions is the number of unique applicants and  {default_caption}")
  ) 
```

```{r}
#| label: multi_compl_deep_dive

#identify which users submitted multiple income summaries
v_multi <- df_mp |> 
  filter(pilot == "Nov 2025") |> 
  filter(event %in% c("ApplicantSharedIncomeSummary")) |> 
  distinct(pilot, distinct_id, cbv_flow_id) |> 
  group_by(distinct_id) |> 
  filter(n() > 1) |> 
  ungroup() |> 
  distinct(distinct_id) |> 
  pull()

#pull the employer information from their sync
df_multi <- df_mp |> 
  filter(
    pilot == "Nov 2025",
    event %in% c("ApplicantFinishedSync"),
    distinct_id %in% v_multi
  ) |> 
  select(pilot, distinct_id, cbv_flow_id, employer_name)

#review entries
df_mult_types <- df_multi |> 
  add_count(distinct_id, employer_name, name = "submission_count") |> 
  group_by(distinct_id) |> 
  summarise(
    unique_sessions = n_distinct(distinct_id, cbv_flow_id),
    unique_events = n(),
    unique_employers = n_distinct(employer_name),
    max_repeats = max(submission_count), 
    .groups = "drop"
  ) |> 
  mutate(
    behavior = case_when(
      unique_employers == 1 ~ "Single item repeated",
      unique_employers == unique_events ~ "All unique items",
      max_repeats > 1 ~ "Multiple items with repeats",
      TRUE ~ "Other"
    )
  ) 
  
  df_multi |> 
    left_join(df_mult_types |> select(distinct_id, behavior)) |> 
    write_csv(here("app/analytics/multi_sub.csv"))

  df_mult_types <- df_mult_types |> 
    count(behavior)

```
We should take a closer look at the repeated submissions. Some of these could be resubmissions of the same information (i.e. the applicant didn't know if the submission went through or forgot they already submitted it) or could be going back to enter in a different employeer at a differnt time. If we look at the `r sum(df_mult_types$n)` applicants with repeated submissions in the November pilot, `r df_mult_types[df_mult_types$behavior == 'Single item repeated',]$n` submitted for the same employer each time, `r df_mult_types[df_mult_types$behavior == 'Multiple items with repeats',]$n` submitted different employers (some with repeats), and only `r df_mult_types[df_mult_types$behavior == 'All unique items',]$n` had all unique employer submissions. 

### Is there a drop in completion as the pilot period draws on?

If we compare weeks with in each pilot, we are seeing a similar trend of most people starting in the first week or two and then tailing off after that. While the first two pilot periods saw the second week as having the highest compeltion rate, the current pilot saw an increase in weeks 2 and then week 3 for completion rates.

```{r}
#| label: retention_viz
#| output: true

 df_compl_wkly <- df_mp |> 
  filter(event %in% c("ApplicantViewedAgreement","ApplicantSharedIncomeSummary")) |> 
  clean_events() |> 
  mutate(week = timestamp |> floor_date("weeks") |> as_date()) |> 
  group_by(pilot, week, event_clean) |> 
  summarise(
    applicants = n_distinct(distinct_id),
    events = n(),
    .groups = "drop"
    ) |>
  pivot_longer(c(applicants, events), names_to = "denom") |> 
  pivot_wider(
    names_from = event_clean,
    values_from = value
    ) |> 
  mutate(
    completion_rate_wk = `Shared Income Summary` / `Viewed Agreement`,
    across(where(is.numeric), ~ ifelse(pilot == "May 2025" & denom == "applicants", NA_integer_, .x))
    ) |> 
  group_by(pilot) |> 
  mutate(plt_wk = glue("wk{isoweek(week) - min(isoweek(week)) + 1}"), .after = pilot) |> 
  ungroup()


# merge on pilot average rate for viz
df_compl_wkly <- df_compl_wkly |> 
  left_join(
    df_compl_pilot |> 
      select(pilot, denom, completion_rate_pilot = `Completion Rate`) |> 
      mutate(plt_wk = 'wk5'),
    by = join_by(pilot, plt_wk, denom)
    )

v1 <- df_compl_wkly |> 
  arrange(plt_wk, desc(denom)) |> 
  ggplot(aes(plt_wk, `Viewed Agreement`, fill = denom)) +
  geom_col(position = position_dodge(-.5), na.rm = TRUE) + 
  facet_grid(~pilot, scales = "free_x", space = "free_x", switch = "y") +
  scale_y_continuous(label = label_comma(scale_cut = cut_short_scale()), breaks = seq(0, 14000, 2000)) +
  scale_fill_manual(values = c(applicants = unname(dsac_color['light_navy']), events = unname(dsac_color['pale_navy']))) +
  si_style_ygrid() +
  labs(
    x = NULL, y = NULL,
    subtitle = glue("Started Application (<span style = color:{dsac_color['light_navy']};>**applicants**</span>/<span style = color:{dsac_color['pale_navy']};>**events**</span>)")) +
  theme(
    strip.text = element_text(hjust = .5),
    strip.placement = "outside",
    panel.spacing = unit(0.5, "lines"),
    legend.position = "none",
    plot.subtitle = element_markdown(),
    )

  v2 <- df_compl_wkly |> 
  ggplot(aes(plt_wk, completion_rate_wk, fill = denom)) +
  geom_col(na.rm = TRUE) +
  geom_hline(
    aes(yintercept = completion_rate_pilot), na.rm = TRUE,
    color = dsac_color[['cranberry']],
    linetype = "dashed"
    ) +
  geom_label(
    aes(
      y = completion_rate_pilot,
      label = label_percent(1)(completion_rate_pilot)), 
    na.rm = TRUE, fill = "white",
    family = "Source Sans 3", color = matterhorn, linewidth = 0,
    ) +
  facet_grid(fct_rev(denom) ~pilot, scales = "free_x", , space = "free_x", switch = "y") +
  scale_y_continuous(label = label_percent()) +
  scale_fill_manual(values = c("events" = "#97b2ae", 'applicants' = unname(dsac_color[['light_teal']]))) + 
  labs(
    x = NULL, y = NULL,
    subtitle = "Application Completion Rate") +
  si_style_ygrid() +
  theme(
    strip.text.y = element_text(hjust = .5, face = "bold"),
    strip.text.x = element_text(hjust = .5),
    strip.placement = "outside",
    panel.spacing = unit(0.5, "lines"),
    legend.position = "none"
    )

    v1 / v2 + 
      plot_annotation(
        title = "LARGE DROP OFF IN INITIATING APPLICATION AFTER THE FIRST WEEK, BUT LARGER COMPLETION RATE IN FOLLOWING WEEKS" |> str_wrap(70),
        caption = default_caption,
        theme = si_style()
      )
```

### Is there a difference in completion based on their device or how they receive the pilot url?

For the Louisana pilot, applicants were sent both text messages and emailed/mailed with a notice about the pilot. We can investigate whether there is any difference in completion rates based on how the applicants are accessing the app.

```{r}
#| label: device_origin_status

#flag if the applicant started on a tokenized or generic link
df_device_origin_status <- df_mp |> 
  mutate(
    initiation = case_when(event %in% c("ApplicantClickedCBVInvitationLink", "ApplicantClickedGenericLink") ~ str_extract(event, "CBVInvitation|ClickedGenericLink")),
    initiation = initiation |> 
      str_replace_all("(?<!^)([A-Z])", " \\1") |> 
      str_replace('C B V', "CBV")
  ) |> 
  group_by(distinct_id, cbv_flow_id) |> 
  fill(initiation, .direction = "updown") |> 
  ungroup() 
  
df_device_origin_status <- df_device_origin_status |> 
  filter(
    str_starts(distinct_id, "applicant"),
    event %in% c("ApplicantViewedAgreement", "ApplicantSharedIncomeSummary")
  ) |>  
  mutate(
    completed = event == "ApplicantSharedIncomeSummary",
    across(c(device_type, origin, initiation), ~ ifelse(is.na(.), "unknown", .)),
    device_type = ifelse(device_type %in% c("desktop", 'smartphone', 'unknown'), device_type, "other")
  ) |> 
  group_by(pilot, distinct_id, cbv_flow_id) |> 
  mutate(completed = max(completed) |> as.logical()) |> 
  ungroup() |> 
  distinct(pilot, distinct_id, cbv_flow_id, device_type, origin, completed, initiation) 

#other device types
other_devices <- unique(df_mp$device_type) |> 
  str_subset("desktop|smartphone", negate = TRUE) |> 
  paste0('s') |> 
  glue_collapse(sep = ', ', last = ", and ")

```

When comparing the device type, we can see there is a preference for applicants initiating the process on their phones. The success rate is much higher on the phones than desktop, but both success rates are rather low. The other types of devices include `{r} other_devices`.

```{r}
#| label: device_status_viz
#| output: true

df_device_status_viz <- df_device_origin_status |>
  group_by(pilot, device_type) |> 
  summarise(
    started = n(),
    #completed = sum(completed, na.rm = TRUE),
    completion_rate = sum(completed, na.rm = TRUE) / n(),
    .groups = "drop"
  ) |> 
    pivot_longer(
      c(started, completion_rate),
      names_to = "type"
    ) |> 
  mutate(
    value_label = ifelse(type == "started", label_comma(1)(value), label_percent(1)(value)),
    fill_color = ifelse(type == "started", dsac_color["light_navy"], dsac_color["light_teal"]),
    type = ifelse(type == "started", "Started Application", "Completion Rate")
    )

df_device_status_viz |> 
  ggplot(aes(value, fct_reorder(device_type, value, sum), fill = fill_color)) +
  geom_col() +
  geom_text(aes(label = value_label), family = "Source Sans 3", color = matterhorn, hjust = 0) +
  facet_grid(pilot~fct_rev(type), scales = "free_x", space = "free_y", switch = "y") +
  coord_cartesian(clip = "off") +
  scale_fill_identity() +
  si_style_xgrid() +
  labs(
    x = NULL, y = NULL,
    title = "MOST APPLICANTS START AND HAVE A HIGHER COMPLETION RATE ON A SMARTPHONE THAN DESKTOP" |> str_wrap(70),
    subtitle = "number of events by device type",
    caption = glue("Note: Other devices include {other_devices} | {default_caption}")
  ) +
  theme(
    legend.position = "none",
    axis.text.x = element_blank(),
    strip.placement = "outside",
    panel.spacing = unit(0.5, "lines"),
    strip.text.y = element_text(hjust = .5)
  )

```

When looking the url origin, we see the large majority arise from SMS messages. However, the success rate is much lower than that of physical mailings.

```{r}
#| label: origin_status_viz
#| output: true

df_origin_status_viz <- df_device_origin_status |>
  group_by(pilot, origin) |> 
  summarise(
    started = n(),
    #completed = sum(completed, na.rm = TRUE),
    completion_rate = sum(completed, na.rm = TRUE) / n(),
    .groups = "drop"
  ) |> 
    pivot_longer(
      c(started, completion_rate),
      names_to = "type"
    ) |> 
  mutate(
    value_label = ifelse(type == "started", label_comma(1)(value), label_percent(1)(value)),
    fill_color = ifelse(type == "started", dsac_color["light_navy"], dsac_color["light_teal"]),
    type = ifelse(type == "started", "Started Application", "Completion Rate"),
    origin = str_replace(origin, "mfb_", "MFB ")
    )

df_origin_status_viz |> 
  filter(pilot != "May 2025") |> 
  ggplot(aes(value, fct_reorder(origin, value, sum), fill = fill_color)) +
  geom_col() +
  geom_text(aes(label = value_label), family = "Source Sans 3", color = matterhorn, hjust = 0) +
  facet_grid(pilot~fct_rev(type), scales = "free", space = "free_y", switch = "y") +
  coord_cartesian(clip = "off") +
  scale_fill_identity() +
  si_style_xgrid() +
  labs(
    x = NULL, y = NULL,
    title = "Mailed links had the highest flow completion rate" |> toupper(),
    subtitle = "number of distinct flows by url link origin",
    caption = glue("Note: Excludes May pilot which did not capture device type
    {default_caption}")
  ) +
  theme(
    legend.position = "none",
    axis.text.x = element_blank(),
    strip.placement = "outside",
    panel.spacing = unit(0.5, "lines"),
    strip.text.y = element_text(hjust = .5)
  )
```

Lastly, we can take a look at how the applicant started their session. In the November pilot, most sessions were started via a tokenized, CBV invitation but these sessions were half as successful in completing the process. This is the opposite of what occured in the August pilot.

```{r}
#| label: tokenized_status
#| output: true

df_link_status_viz <- df_device_origin_status |>
  group_by(pilot, initiation) |> 
  summarise(
    started = n(),
    #completed = sum(completed, na.rm = TRUE),
    completion_rate = sum(completed, na.rm = TRUE) / n(),
    .groups = "drop"
  ) |> 
    pivot_longer(
      c(started, completion_rate),
      names_to = "type"
    ) |> 
  mutate(
    value_label = ifelse(type == "started", label_comma(1)(value), label_percent(1)(value)),
    fill_color = ifelse(type == "started", dsac_color["light_navy"], dsac_color["light_teal"]),
    type = ifelse(type == "started", "Started Application", "Completion Rate")
    )

df_link_status_viz |> 
  ggplot(aes(value, fct_reorder(initiation, value, sum), fill = fill_color)) +
  geom_col() +
  geom_text(aes(label = value_label), family = "Source Sans 3", color = matterhorn, hjust = -.1) +
  facet_grid(pilot~fct_rev(type), scales = "free", space = "free_y", switch = "y") +
  coord_cartesian(clip = "off") +
  scale_fill_identity() +
  si_style_xgrid() +
  labs(
    x = NULL, y = NULL,
   title = "GENERIC LINKS SAW A LARGER SUCCESS RATE LAST TWO PILOTS"|> str_wrap(70),
    subtitle = "number of distinct flows by initiation type",
    caption = default_caption
  ) +
  theme(
    legend.position = "none",
    axis.text.x = element_blank(),
    strip.placement = "outside",
    panel.spacing.x = unit(0.5, "lines"),
    panel.spacing.y = unit(1, "lines"),
    strip.text.y = element_text(hjust = .5)
  )
```

## How long does it take to complete the application?

On the client side, the goal of the pilot application is to creating something easy to complete. Ideally, applicants can complete the process in a matter of minutes. We can look at time it takes to submit to understand if it places an undue burden on applicants. 

```{r}
#| label: duration_calc

df_time <- df_mp |> 
  filter(str_starts(distinct_id, "applicant")) |> 
  mutate(
    completed = event == "ApplicantSharedIncomeSummary", 
    time_start = case_when(event == "ApplicantViewedAgreement" ~ timestamp)
    ) |> 
  group_by(pilot, distinct_id) |> 
  # group_by(pilot, distinct_id, cbv_flow_id) |> 
  mutate(
    completed = max(completed, na.rm = TRUE),
    time_end = ifelse(event == "ApplicantSharedIncomeSummary", timestamp, max(timestamp)),
    time_end = as_datetime(time_end)
    ) |>
  ungroup()

df_duration <- df_time |> 
  filter(!is.na(time_start)) |> 
  group_by(pilot, distinct_id) |> 
  summarise(
    # sessions = n_distinct(cbv_flow_id),
    completed = max(completed, na.rm = TRUE),
    time_start = max(time_start, na.rm = TRUE),
    time_end = max(time_end, na.rm = TRUE),
    .groups = "drop"
    ) |> 
  # tidylog::filter(time_start != time_end) |> 
  mutate(duration = time_end - time_start)
```

```{r}
#| label: duration_stats

df_duration_viz <- df_duration |> 
  mutate(
    completed_type = case_when(
      completed == 1 ~ 'completed',
      time_start == time_end ~ 'just clicked',
      TRUE ~ 'incomplete'
      ),
    minutes = as.numeric(duration) / 60  
      ) |> 
  group_by(pilot, completed_type) |> 
  summarise(
    n = n(),
    q25 = quantile(minutes, 0.25, na.rm = TRUE),
    median = median(minutes, na.rm = TRUE),
    mean = mean(minutes, na.rm = TRUE),
    q75 = quantile(minutes, 0.75, na.rm = TRUE),
    q90 = quantile(minutes, .9, na.rm = TRUE),
    .groups = "drop"
    ) |> 
    mutate(
      fill_color = 
        case_match(completed_type,
        'completed' ~ dsac_color['light_navy'],
        'incomplete' ~ dsac_color['light_cranberry'],
        'just clicked' ~ dsac_color['light_cranberry']
        ),
      completed_type_lab = glue('{completed_type} (n = {label_comma()(n)})')
    )

  df_duration_stats <- df_duration_viz |> 
    filter(pilot == 'Nov 2025')
```

Looking specifically at the November pilot, of the `{r} scales::label_comma()(df_duration_stats[df_duration_stats$completed_type == 'completed',]$n)` applicants who completed the application, the median time was `{r} scales::label_number(.1)(df_duration_stats[df_duration_stats$completed_type == 'completed',]$median)` minutes, with 75% completing it within `{r} scales::label_number(.1)(df_duration_stats[df_duration_stats$completed_type == 'completed',]$q75)` minutes. The timing looks pretty good from that vantage point. However, most applicants (`{r} scales::label_comma()(df_duration_stats[df_duration_stats$completed_type != 'completed',]$n |> sum())`) are not completing the application. Of those applicants, `{r} scales::label_percent()(df_duration_stats[df_duration_stats$completed_type == 'just clicked',]$n / df_duration_stats[df_duration_stats$completed_type != 'completed',]$n |> sum())` are clicking the link and not proceeding beyond viewing the initial agreement page. When looking at incomplete applications excluding those that only viewed the agreement page, the median applicant spends `{r} scales::label_number(.1)(df_duration_stats[df_duration_stats$completed_type == 'incomplete',]$median)` minutes on the application before they stop, with 75% aborting after only `{r} scales::label_number(.1)(df_duration_stats[df_duration_stats$completed_type == 'incomplete',]$q75)` minutes. 

```{r}
#| label: viz_duration
#| output: true
df_duration_viz |>  
  ggplot(aes(y = fct_rev(completed_type_lab), group = completed_type, color = fill_color)) +
  geom_linerange(aes(xmin = q25, xmax = q75)) +
  geom_point(aes(median), size = 4) +
  geom_text(
  data  = df_duration_viz |> filter(median != 0),
    aes(median, label = label_number(accuracy = .1, suffix = 'm')(median)), 
    family = "Source Sans 3", vjust = -1) +
  facet_grid(pilot ~. , switch = "y", scales = "free_y") +
  scale_x_continuous(labels = label_number( suffix = 'm'), position = "top") +
  scale_color_identity() +
  coord_cartesian(clip = "off") +
  labs(
    x = NULL, y = NULL,
    title = glue("{toupper(df_duration_stats$pilot[1])} INCOMPLETE APPLICATIONS END AFTER ABOUT {label_number(.1)(df_duration_stats[df_duration_stats$completed_type == 'incomplete',]$median)} MINUTES") |> str_wrap(70),
    subtitle = 
      "displaying the median session **minutes** and IQR",
    caption = glue("Note: Applicants falling into 'just clicked' count represent those who didn't have an event beyond viewing the agreement 
    {default_caption}") 
      ) +
  si_style() +
  theme(
    plot.subtitle = element_markdown(),
    strip.placement = "outside",
    strip.text = element_text(hjust = .5),
    panel.spacing=unit(0.5, "lines")
    )

```

### How long does the income data syncing take?

Syncing the data once the employer is chosen can take a bit of time to occur. The longer it takes, we may be more likely to see applicants drop off.  

```{r}
#| label: data_sync

df_sync_init <- df_mp |> 
  filter(
    event %in% c("ApplicantAttemptedLogin","ApplicantSucceededWithLogin", "ApplicantFinishedSync"),
    ) |>
  distinct(distinct_id, cbv_flow_id, timestamp, event, pilot, provider) |> 
  arrange(distinct_id, cbv_flow_id, timestamp) |> 
  group_by(distinct_id, cbv_flow_id, event) |>
  mutate(attempt_n = row_number()) |> 
  ungroup() |>
  pivot_wider(
    names_from = event,
    values_from = timestamp
    ) |> 
  mutate(sync_time = ApplicantFinishedSync - ApplicantSucceededWithLogin)

df_sync <- df_sync_init |> 
  tidylog::filter(
    !is.na(sync_time), 
    sync_time >= 0
    ) 

df_sync_viz <- df_sync |> 
  group_by(pilot, provider) |> 
  summarise(
    n = n(),
    min = min(sync_time, na.rm = TRUE), 
    q25 = quantile(sync_time, 0.25, na.rm = TRUE),
    median = median(sync_time, na.rm = TRUE),
    q75 = quantile(sync_time, 0.75, na.rm = TRUE),
    max = max(sync_time, na.rm = TRUE),
    .groups = "drop"
    ) 

```
```{r}
#| label: long_sync

lng_run <- 180

v_lng_run_shr <- df_sync |> 
  count(sync_time > lng_run) |> 
  mutate(share = n / sum(n)) |> 
  filter(`sync_time > lng_run` == TRUE) |> 
  mutate(label = label_percent(1)(share)) |> 
  pull()

```
Overall, we are looking at a median sync time of `{r} median(df_sync$sync_time, na.rm = TRUE)` seconds, with 75% of events seeing it take under `{r} quantile(df_sync$sync_time, .75, na.rm = TRUE)` seconds. Of the `{r} scales::label_comma()(nrow(df_sync))` successful data syncs, `{r} v_lng_run_shr` take longer than `{r} lng_run` seconds to sync. 

We can see how this places our when comparing different payroll aggregators used, Argyle and Pinwheel. Each pilot we can see there is a signficant gap between the time range it takes to sync with Pinwheel versus Argyle. 

```{r}
#| label: data_sync_tbl
#| eval: false

df_sync_viz |> 
  gt(groupname_col = "pilot") |> 
  fmt_number(columns = everything(), decimals = 0) |> 
  fmt_duration(columns = -n,
    input_units = "seconds"
    ) |> 
  adjust_row_padding() |> 
  si_gt_base(header_fill = dsac_color['light_teal']) |> 
  add_labs(
    caption = default_caption
  )

```

```{r}
#| label: data_sync_viz
#| output: true
df_sync_viz |> 
  mutate(
    fill_color = ifelse(provider == "Argyle", dsac_color['light_navy'], dsac_color['light_teal']),
    provider = glue('{provider} (n = {label_comma()(n)})')) |> 
  ggplot(aes(y = fct_rev(provider), group = provider, color = fill_color)) +
  geom_linerange(aes(xmin = q25, xmax = q75)) +
  geom_point(aes(median), size = 4) +
  geom_text(
    aes(median, label = label_timespan(accuracy = 1)(median)), 
    family = "Source Sans 3", vjust = -1) +
  facet_grid(pilot ~. , switch = "y", scales = "free_y") +
  scale_x_continuous(label = label_timespan(), position = "top") +
  scale_color_identity() +
  labs(
    x = NULL, y = NULL,
    title = "Slightly shorter Pinwheel median sync, but a smaller sample size" |> toupper(),
    subtitle = 
      "displaying the median session **seconds** against the IQR",
    caption = default_caption 
      ) +
  si_style_xgrid() +
  theme(
    strip.placement = "outside",
    strip.text = element_text(hjust = .5),
    plot.subtitle = element_markdown(),
    panel.spacing=unit(0.5, "lines")
    )
```

Sticking with the provider comparision, we can look at the distribtion of sync times. By bucketing them, we can see that most events have a sync time between 30 seconds and a minute (first 2 rows, green and dark blue bars) and about 95% are under the 3 minute mark (first 3 rows - green, dark blue, and blue columns). 

```{r}
#| label: data_sync_bucketed_viz
#| output: true
df_sync_bucketed <- df_sync |> 
  mutate(
    sync_bucket = 
      case_when(
        sync_time <= 30 ~ "<=30s",
        sync_time <= 60 ~ ">30s - 1m",
        sync_time <= 180 ~ ">1m - 3m",
        sync_time <= 300 ~ ">3m - 5m",
        sync_time <= 600 ~ ">5m - 10m",
        TRUE ~ ">10m"
        ),
      sync_bucket = factor(sync_bucket, c("<=30s", ">30s - 1m", ">1m - 3m", ">3m - 5m", ">5m - 10m", ">10m"))
  ) |> 
  count(pilot, sync_bucket) |> 
  arrange(pilot, sync_bucket) |> 
  group_by(pilot) |> 
  mutate(
    share = n / sum(n),
    cum_share = cumsum(n) / sum(n)
    ) |> 
  ungroup()

  df_sync_bucketed |> 
    ggplot(aes(n, fct_rev(sync_bucket), fill = sync_bucket)) +
    geom_col() + 
    geom_text(
      aes(label = label_percent(1)(share)),
      family = "Source Sans 3", hjust = -.1, color = matterhorn, 
      ) + 
    facet_wrap(~pilot) +
    coord_cartesian(clip = "off") + 
    scale_fill_manual(values = unname(c(dsac_color['teal'], dsac_color['dark_navy'], dsac_color['navy'], dsac_color['light_navy'], dsac_color['light_cranberry'], dsac_color['cranberry']))) +
    labs(
      x = NULL, y = NULL,
      title = "95% OF SYNC TIMES ARE UNDER THREE MINUTES",
      subtitle = "number of sessions' income sync time",
      caption = default_caption) +
    si_style_xgrid() +
    theme(legend.position = "none")    
```

The next visual looks at the same data, but plots each of the event individually, allowing us to see some of the larger outliers and where the specific times fall compared within the broader buckets. The x-axis has been truncated to 20 minutes to see the distrbution, plotting the `r dplyr::filter(df_sync, sync_time > 1200) |> nrow()` outliers on the 20 minute mark. Both providers saw three events with sync times over 20 minutes in the November pilot, but the median time went down from August.  

```{r}
#| label: data_sync_distro_viz
#| output: true 
 
 df_sync |>
    mutate(
      sync_bucket = 
        case_when(
          sync_time <= 30 ~ "<=30s",
          sync_time <= 60 ~ ">30s - 1m",
          sync_time <= 180 ~ ">1m - 3m",
          sync_time <= 300 ~ ">3m - 5m",
          sync_time <= 600 ~ ">5m - 10m",
          TRUE ~ ">10m"
          ),
      sync_bucket = factor(sync_bucket, c("<=30s", ">30s - 1m", ">1m - 3m", ">3m - 5m", ">5m - 10m", ">10m"))) |> 
    ggplot(aes(sync_time, fct_rev(pilot), color = sync_bucket)) +
    geom_point(position = position_jitter(height = 0.3, seed = 42), alpha = 0.4) + 
    stat_summary(fun = "median", color = dsac_color['gold'], shape = 23, linewidth = 1.1, na.rm = TRUE) + 
    facet_grid(provider ~. , switch = "y", scales = "free_y") + 
    scale_x_continuous(
      label = label_number(.1, scale = 1/60, suffix = "m"), limits = c(0, 1200),
      breaks = seq(0, 1200, 150),
      oob=scales::squish, position = "top") +
    scale_color_manual(values = unname(c(dsac_color['teal'], dsac_color['dark_navy'], dsac_color['navy'], dsac_color['light_navy'], dsac_color['light_cranberry'], dsac_color['cranberry']))) +
    labs(
      x = NULL, y = NULL, 
      title = "MEDIAN SYNC TIME IMPROVED IN NOVEMBER PILOT",
      subtitle = "income sync time with provider | each point represents a session | gold diamonds represent the median" |> str_wrap(),
      caption = glue("Note: Sync time capped at 20 minutes
      {default_caption}")) +
    si_style_xgrid() +
    theme(
      strip.text = element_text(hjust = .5),
      strip.placement = "outside",
      legend.position = "none"
      )
```
### When there was a provider outage, was there any change in processing times?

During the November pilot, Argyle had an outage between November 19-20. Looking by day, we can see that Monday, November 20th had a dip in the number of Argyle events compared with the prior day and Pinhweel saw an increase on both November 19th and 20th (though certainly not enough to offset the dip). During the first day of the outage, the success rate of logging for both providers dropped precipitously, but rebounded on November 20th.

```{r}
#| label: sync_outage
#| output: true
#| fig-height: 7

df_outage_viz_rt <- df_sync_init |> 
  filter(
    pilot == "Nov 2025",
    !is.na(ApplicantAttemptedLogin),
    ) |> 
  mutate(
    date = as_date(ApplicantAttemptedLogin),
    succeeded = !is.na(ApplicantFinishedSync)
  ) |> 
  group_by(date, provider, distinct_id, cbv_flow_id) |> 
  summarize(succeeded = max(succeeded, na.rm = TRUE), .groups = "drop") |> 
  group_by(date, provider) |> 
  summarize(
    attempted = n_distinct(distinct_id, cbv_flow_id),
    succeeded = sum(succeeded, na.rm = TRUE),
    .groups = "drop"
  ) |> 
  mutate(
    success_rate = succeeded/attempted,
    fillcolor = ifelse(provider == "Argyle", dsac_color['light_navy'], dsac_color['light_teal']),
    provider_x = ifelse(provider == "Argyle", -.08, -.05),
    date_fmt = case_when(
      date %in% c('2025-11-17', '2025-12-01') ~ format(date, format = "%b %d"),
      month(date) == 11 ~ format(date, format = "%d"),
      month(date) == 12 ~ format(date, format = " %d"),
      ) |> fct_inorder()
    )

df_outage_viz_rt |> 
  ggplot(aes(y = fct_rev(date_fmt), color = fillcolor, , fill = fillcolor, group = date_fmt)) +
  geom_line(aes(success_rate), color = "#909090") + 
  geom_point(aes(success_rate), size = 3, shape = 21, color = "white") +
  geom_point(aes(provider_x, size = attempted), shape = 15) +
  geom_text(
    aes(provider_x - .02, label = attempted),
    data = df_outage_viz_rt |> filter(provider == "Argyle"), 
    family = "Source Sans 3", hjust = 1, color = "#505050") + 
  geom_text(
    aes(provider_x + .04, label = attempted),
    data = df_outage_viz_rt |> filter(provider != "Argyle"), 
    family = "Source Sans 3", hjust = 1, color = "#505050") + 
  annotate(
    geom = 'rect',
    ymin = 29.5, ymax = 31.5,
    # ymin = '2025-11-19', ymax = '2025-11-19',
    xmin = -Inf, xmax = Inf,
    alpha = .3, fill = "#909090"
    ) +
  scale_x_continuous(
    labels = label_percent(), 
    breaks = seq(0, 1, .2), 
    minor_breaks = seq(0, 1, .1),
    position = "top") + 
  scale_color_identity(aesthetics = c('fill', 'color')) +
  scale_size(guide = "none") +
  labs(
    x = NULL, y = NULL,
    title = "ARGYLE USERS TYPICALLY SAW A HIGHER SUCCESSFUL LOGIN RATE",
    subtitle = glue("daily <span style = color:{dsac_color['light_navy']};>**Argyle**</span>/<span style = color:{dsac_color['light_teal']};>**Pinwheel**</span> user flow attempts to login [left] and  successful rates [right]"),
    caption = glue('Note: Argyle outage Nov 19-20
    {default_caption}')
  ) + 
  si_style_xgrid() +
  theme(plot.subtitle = element_markdown())

#si_save(here("app/analytics/success_rate.png"), height = 7)
```

During those two Argyle outage days, Pinwheel had the two highest median data sync times for the fives weeks of the pilot. 

```{r}
#| label: sync_outage2
#| output: true
#| fig-height: 7

df_outage_viz <- df_sync |> 
  filter(
    pilot == "Nov 2025",
    !is.na(ApplicantAttemptedLogin)
    ) |> 
  mutate(date = ApplicantAttemptedLogin |> as_date()) |> 
  group_by(date, provider) |> 
  summarize(
    n = n_distinct(distinct_id, cbv_flow_id),
    q25 = quantile(sync_time, 0.25, na.rm = TRUE),
    median = median(sync_time, na.rm = TRUE),
    q75 = quantile(sync_time, 0.75, na.rm = TRUE),
    .groups = "drop"
    ) |> 
  mutate(
    fillcolor = ifelse(provider == "Argyle", dsac_color['light_navy'], dsac_color['light_teal']),
    provider_x = ifelse(provider == "Argyle", -35, -20),
    date_fmt = case_when(
      date %in% c('2025-11-17', '2025-12-01') ~ format(date, format = "%b %d"),
      month(date) == 11 ~ format(date, format = "%d"),
      month(date) == 12 ~ format(date, format = " %d"),
      ) |> fct_inorder()
  )

df_outage_provider_median <- df_sync |> 
  filter(
    pilot == "Nov 2025",
    !is.na(ApplicantAttemptedLogin)
    ) |> 
  mutate(date = ApplicantAttemptedLogin |> as_date()) |> 
  group_by(provider) |> 
  summarize(median_provider = median(sync_time, na.rm = TRUE) |> as.numeric()) |> 
  ungroup() |> 
  pivot_wider(names_from = provider, values_from = median_provider)

df_outage_viz |> 
  ggplot(aes(y = fct_rev(date_fmt), color = fillcolor, group = provider)) +
  geom_linerange(aes(xmin = q25, xmax = q75), position = position_dodge(.9,  orientation = "y")) +
  geom_point(aes(provider_x, size = n), shape = 15) + 
  geom_text(
    aes(provider_x -10, label = n),
    data = df_outage_viz |> filter(provider == "Argyle"), 
    family = "Source Sans 3", hjust = 1, color = "#505050") + 
  geom_text(
    aes(provider_x + 14, label = n),
    data = df_outage_viz |> filter(provider != "Argyle"), 
    family = "Source Sans 3", hjust = 1, color = "#505050") + 
  geom_point(aes(median), position = position_dodge(.9, orientation = "y")) +
  annotate(
    geom = 'rect',
    ymin = 29.5, ymax = 31.5,
    xmin = -Inf, xmax = Inf,
    alpha = .3, fill = "#909090"
    ) + 
  scale_x_continuous(
    limit = c(-45,300), 
    label = label_number(1, scale = 1/60, suffix = "m"),
    breaks = seq(0, 300, 60),
    oob = scales::squish, 
    position = "top") + 
  scale_color_identity() +
  scale_size(guide = "none") +
  labs(
    x = NULL, y = NULL,
    title = glue("DESPITE SOME LARGE OUTLIERS AND LESS FLOWS, PINWHEEL'S MEDIAN SYNC TIME ({df_outage_provider_median$Pinwheel}s) IS ONLY A FEW SECONDS SLOWER THAN ARGYLE ({df_outage_provider_median$Argyle}s)") |> str_wrap(70),
    subtitle = glue("successful daily <span style = color:{dsac_color['light_navy']};>**Argyle**</span>/<span style = color:{dsac_color['light_teal']};>**Pinwheel**</span> user flow syncs [left] and their duration, **minutes** [median + IQR]"),
    caption = glue('Note: x axis truncated to 5 minutes; Argyle outage Nov 19-20 
    {default_caption}')
  ) + 
  si_style_xgrid() +
  theme(plot.subtitle = element_markdown())

#si_save(here("app/analytics/sync_times.png"), height = 7)
```

## Where are applicants dropping off?

To get a sense of where applicants are dropping off, we can look at some key points/pages to see whether they reached that step. The Sankey diagram below depicts these key points in the November 2025 pilot. 

```{r}
#| label: event_steps

v_steps <- c(
  "ApplicantViewedAgreement",
  "ApplicantAgreed",
  "ApplicantSelectedEmployerOrPlatformItem",
  "ApplicantAttemptedLogin",
  "ApplicantSucceededWithLogin",
  "ApplicantViewedPaymentDetails",
  "ApplicantSharedIncomeSummary"
  )

v_steps_clean <- 
  tibble(event = v_steps) |> 
  add_row(event = "dropped", .before = 1) |> 
  clean_events() |> 
  pull()

v_steps_breaks <- str_replace_all(v_steps_clean, " ", "\n")
```

```{r}
#| label: event_journey

#setup full set of steps for each applicant to id drop off point
df_journey <- df_mp |> 
  filter(
    pilot == "Nov 2025", 
    event %in% v_steps) |> 
  mutate(
    event = factor(event, v_steps),
    status = "reached"
  ) |> 
  select(pilot, distinct_id, event, status) |> 
  distinct(pilot, distinct_id, event, status) |> 
  complete(pilot, distinct_id, event) |> 
  arrange(distinct_id, event) |> 
  clean_events()

#apply step order # and label drop point
df_journey <- df_journey |> 
  group_by(pilot, distinct_id) |> 
  mutate(
    step_order = row_number(),
    step_event =  case_when(
      status == "reached" ~ as.character(event_clean), 
      is.na(status) & lag(status) == "reached" ~ "dropped"
    )) |>
  ungroup() %>%
  filter(!is.na(step_event)) |> 
  select(-c(event, event_clean, status)) |> 
  mutate(step_event = factor(step_event, v_steps_clean))

#value labels to merge on with long dataset
df_journey_label <- df_journey |> 
  count(step_order, step_event) |> 
  mutate(step_order = glue("step_{step_order}") |> as.character()) |> 
  group_by(step_order) |> 
  mutate(share = n / sum(n)) |> 
  ungroup()

#spread to setup for sankey plot
df_journey <- df_journey |> 
  pivot_wider(
    names_from = step_order,
    values_from = step_event,
    names_prefix = "step_"
  )


# Convert to long format for ggsankey
df_sankey <- df_journey %>%
  make_long(step_1, step_2, step_3, step_4, step_5, step_6, step_7) |> 
  filter(!is.na(node)) |> 
  mutate(
    node = factor(node, v_steps_clean),
    next_node = factor(next_node, v_steps_clean),
    fill_color = ifelse(node == "dropped", dsac_color[['light_cranberry']], dsac_color[['light_navy']])
   )

#apply value for labeling
df_sankey <- df_sankey |> 
  left_join(
    df_journey_label,
    by = join_by(x == step_order, node == step_event)
  ) |> 
  mutate(value_label = ifelse(x == "step_1", label_comma()(n), glue("{label_comma()(n)}\n{label_percent(1)(share)}")))

```

```{r}
#| label: event_journey_sankey
#| output: true

#viz
df_sankey |> 
  ggplot(
    aes(x = x, 
        next_x = next_x, 
        node = node, 
        next_node = next_node,
        fill = fill_color,
        label = value_label
      )) +
  geom_sankey(flow.alpha = 0.2, flow.fill = matterhorn, node.color = "gray30", width = 0.1, na.rm = TRUE) +
  geom_sankey_label(
    size = 4, color = "white",  hjust = 1.4,
    family = "Source Sans 3", na.rm = TRUE,
  ) +
  scale_x_discrete(labels = v_steps_breaks[2:8], position = "top") +
  scale_fill_identity() + 
  coord_cartesian(clip = 'off') +
  labs(
    x = NULL, y = NULL,
    title = "Significant drop off in applicants agreeing to the terms" |> toupper(),
    subtitle = glue("Number/share of Nov 2025 applicants <span style = color:{dsac_color['light_cranberry']};>dropping off</span> at key points in the application process"),
    caption = default_caption
   ) +
  si_style_nolines() + 
  theme(
    axis.text.y = element_blank(),
    axis.text.x = element_markdown(),
    plot.subtitle = element_markdown()
  ) 
```
Right off the bat, we can see that off the `{r} scales::label_comma()(df_journey_label[df_journey_label$step_order == "step_1",]$n)` applicants who started the process, only `{r} scales::label_percent(1)(df_journey_label[df_journey_label$step_event == "Agreed",]$share)` agreed to terms to move on, and only `{r} scales::label_percent()(df_journey_label[df_journey_label$step_event == "Shared Income Summary",]$n/df_journey_label[df_journey_label$step_order == "step_1",]$n)` actually made it to completion, submitting their pay information. The diagram also show that about 1 in 3 applicants is dropping off when selecting their employeerer, logging in, and then succeeding with that log in. 

The table below so similar drop off rates at each step for each of the three pilots in Lousiana.

```{r}
#| label: event_table
#| output: true

df_ret <- df_mp |>
  filter(event %in% v_steps) |> 
  mutate(event = factor(event, v_steps)) |> 
  arrange(pilot, event) |> 
  clean_events() |> 
  mutate(event_clean = fct_inorder(event_clean)) |> 
  select(pilot, distinct_id, event_clean) |> 
  distinct(pilot, distinct_id, event_clean) |> 
  count(pilot, event_clean, sort = TRUE, name = "events") |> 
  group_by(pilot) |> 
  mutate(
    share_lag = events / lag(events),
    share_cumlative = events / max(events),
    share_cumlative = ifelse(event_clean == "Viewed Agreement", NA, share_cumlative)
      ) |> 
  ungroup() |> 
  arrange(pilot, event_clean) |> 
  mutate(event_clean = as.character(event_clean))

df_ret |>
  gt(groupname_col = "pilot") |> 
  cols_label(
    event_clean = "", 
    share_lag = "Share of Prior Event",
    share_cumlative = "Share of Those Starting"
  ) |> 
  si_gt_base(header_fill = dsac_color['light_teal']) |> 
  adjust_row_padding() |> 
  format_numeric_columns() |> 
  sub_missing()

```




## Are applicants accessing the documentation (help or FAQ) before dropping off?

The prior section gave use a sense of where applicants were dropping off. It may be useful to see if applicants are accessing the documentation before they abort the application. Looking at the data, we can see that on average, those who completed the application had slight more "help actions" than those who did not. This of course is affected by the fact that those who continued saw more pages than those who did not or that using the help more they were more likely to find what they needed to continue onward. 

```{r}
#| label: help_events
v_help_events <- c(
  "ApplicantOpenedHelpModal",
  "ApplicantViewedHelpText", 
  "ApplicantViewedHelpTopic", 
  "ApplicantManuallySwitchedLanguage")
```

```{r}
#| label: help_actions_summary

df_help_actions <- df_time |> 
  mutate(
    completed = as.logical(completed),
    help = event %in% v_help_events,
    started = event == "ApplicantViewedAgreement", #needed for Aug pilot
    distinct_id_started = case_when(started == TRUE ~ distinct_id),
    cbv_flow_id_started = case_when(started == TRUE ~ cbv_flow_id),
    distinct_id_help = case_when(event %in% v_help_events ~ distinct_id),
    ) |> 
  group_by(pilot, completed) |> 
  summarize(
    applicants = n_distinct(distinct_id_started, na.rm = TRUE),
    applicants_with_help_actions = n_distinct(distinct_id_help, na.rm = TRUE),
    sessions = n_distinct(distinct_id_started, cbv_flow_id_started, na.rm = TRUE), 
    help_actions = sum(help, na.rm = TRUE),
    .groups = "drop"
  ) |> 
  mutate(
    help_rate_per_app = help_actions / applicants,
    help_rate_per_sess = help_actions / sessions,
    click_rate_probability = applicants_with_help_actions / applicants
    )

```

```{r}
#| label: help_actions_viz
#| output: true

df_help_actions |> 
  select(pilot, completed, help_rate_per_app, help_rate_per_sess, click_rate_probability) |> 
  pivot_longer(-c(pilot, completed), names_to = "metric") |> 
  mutate(
    metric = 
      case_match(
        metric,
        "click_rate_probability" ~ "**Help Button Click Probability (%)**<br>(help actions / applicants)",
        "help_rate_per_app" ~ "**Help Button Clicks per 100 Applicants**<br>(help actions / applicants)",
        "help_rate_per_sess" ~ "**Help Button Clicks per 100 Sessions**<br>(help actions / sessions)"
        ),
    mark_color = ifelse(completed == TRUE, dsac_color['light_navy'], dsac_color['light_cranberry']),
    mark_fill = ifelse(completed == TRUE, dsac_color['light_navy'], "white") 
  ) |> 
  ggplot(aes(value, fct_rev(pilot), color = mark_color, fill = mark_fill, group = pilot)) +
  geom_path(color = matterhorn) +
  geom_point(size = 4, shape = 21, stroke = 1.1) +
  geom_text(aes(label = label_number(1, scale = 1*100)(value)), family = "Source Sans 3", color = matterhorn, vjust = -1) +
  facet_wrap(. ~ metric, ncol = 1, strip.position = "top") +
  scale_fill_identity() +
  scale_color_identity() +
  scale_x_continuous(labels = label_number(scale = 1*100)) + 
  si_style_xgrid() +
  labs(
    x = NULL, y = NULL,
    title = "HELP USAGE INCREASED EACH PILOT",
    subtitle = glue("Comparing usage across applicants/sessions that were <span style = color:{dsac_color['light_cranberry']};>**incomplete**</span> vs <span style = color:{dsac_color['light_navy']};>**completed**</span>"),
    caption = glue("Note: A help action is when an applicant accesses the help, FAQ, or language change
    {default_caption}")
    ) +
  theme(
    panel.spacing = unit(.5, "lines"),
    axis.text.x = element_blank(),
    strip.text.y = element_text(hjust = .5),
    strip.placement = "outside",
    plot.title = element_markdown(),
    plot.subtitle = element_markdown(),
    strip.text = element_markdown(),
    legend.position = "none"
  )

```

We can investigate this further, looking at the total help actions by page, comparing whether the applicant continued on past that page or ended there. We can see that most of the agreement page are on the agreement. There is significantly more usage of the help^[In this particular case, we are looking at the number of times the help or FAQ was access, so we can have multiple help access points by applicant on the same page] than those who used help on that page but continued on to the next page. 

```{r}
#| label: help_viz
#| output: true

v_page_order <- tibble(event = c("LandingPage", v_steps)) |> clean_events() |> pull()

df_help <- df_time |> 
  left_join(
    tibble(
      event_order = 1:length(v_steps), 
      event = v_steps
      ) |> 
    clean_events() |> 
    rename(page = event_clean),
    by = "event"
  ) |>
  group_by(pilot, distinct_id, cbv_flow_id) |> 
  fill(page, .direction = "up") |> 
  fill(event_order, .direction = "up") |> 
  mutate(
    event_order = ifelse(is.na(event_order), 0, event_order),
    page = ifelse(is.na(page), "Landing Page", page)
    ) |> 
  mutate(
    page = factor(page, v_page_order),
    last_page = event_order == max(event_order, na.rm = TRUE),
    accessed_help = event %in% v_help_events
      ) |> 
  ungroup() |> 
  group_by(pilot, distinct_id, cbv_flow_id, page, last_page) |> 
  mutate(accessed_help = max(accessed_help, na.rm = TRUE)) |> 
  ungroup()


df_help_viz <- df_help  |> 
  mutate(
    started = event == "ApplicantViewedAgreement", #needed for Aug pilot
    distinct_id_started = case_when(started == TRUE ~ distinct_id),
    cbv_flow_id_started = case_when(started == TRUE ~ cbv_flow_id),
    distinct_id_help = case_when(event %in% v_help_events ~ distinct_id),
  ) |> 
  group_by(pilot, page, last_page) |> 
  summarise(
    n_users = n_distinct(distinct_id_started),
    sessions = n_distinct(distinct_id_started, cbv_flow_id_started),
    accessed_help = sum(accessed_help, na.rm = TRUE),
    .groups = "drop"
  ) |> 
  mutate(
    fill_color = ifelse(last_page == TRUE, dsac_color['light_cranberry'], dsac_color['light_navy']),
    last_page_lab = ifelse(last_page == TRUE, "Last Page", "Continued On")
    )

df_help_viz |> 
  ggplot(aes(accessed_help, fct_rev(page), color = fill_color)) +
  geom_linerange(
    aes(xmin = 0, xmax = accessed_help),
    position = position_dodge(.5,  orientation = "y")
    ) + 
  geom_point(position = position_dodge(.5,  orientation = "y")) +
  facet_wrap(pilot~.) +
  labs(
    x = NULL, y = NULL, 
    title = "Help actions higher for applicants continuing" |> toupper(),
    subtitle = glue("Help button clicks comparing applicants who <span style = color:{dsac_color['light_cranberry']};>**ended on that page**</span> or <span style = color:{dsac_color['light_navy']};>**continued on**</span>"),
    caption = default_caption
    ) +
  scale_color_identity() + 
  scale_fill_identity() + 
  scale_x_continuous(label = label_number(scale_cut = cut_short_scale())) +
  si_style_xgrid() +
  theme(plot.subtitle = element_markdown())

```


## How many applicants are not finding their employer in the search?

One reason for retention to drop off could be from applicant not able to find their employers or platforms in in the search.
-
```{r}
#| label: search
#| output: true

df_search <- df_mp |> 
  mutate(
    picked = event == "ApplicantClickedPopularAppEmployers",
    searches = event == "ApplicantSearchedForEmployer",
    missingresults = event == "ApplicantAccessedMissingResultsPage"
    ) |> 
  group_by(pilot) |> 
  summarise(
    n_users = n_distinct(distinct_id),
    sessions = n_distinct(distinct_id, cbv_flow_id),
    picked = sum(picked, na.rm = TRUE),
    searches = sum(searches, na.rm = TRUE),
    missingresults = sum(missingresults, na.rm = TRUE),
    .groups = "drop"
    )

df_search |> 
  gt() |> 
  adjust_row_padding() |> 
  si_gt_base() |> 
  format_numeric_columns()
```

## Are the popular app employers the right ones (i.e. do there appear to be employeers searched more frequently than any that appear in the popular links)

```{r}
# df_employers <- df_mp |> 
#   filter(event== "ApplicantFinishedSync") |> 
#   mutate(
#         employer = map_chr(
#           properties,
#           ~ pluck(.x, "employment_employer_name", .default = NA_character_)
#           )
#         )

# df_employers |> 
#   filter(!is.na(employer)) |> 
#   count(employer, sort = TRUE)
```
